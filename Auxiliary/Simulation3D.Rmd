---
title: "MOTRL Simulation 3D"
author: "Yao Song"
date: "7/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(randomForest)
library(wakefield)
library(dplyr)
library(psych)
library(psychTools)
library(readr)
library(scales)
library(rgl)
library(graphics)
library(lattice)
library(plot3D)
library(tidyr)
library(plot3Drgl)
```

# Functions for simulation.

```{r}
# 1. function to summarize simulation results
summary2 <- function(x) { # x is a numerical vector or matrix
  s1 <- summary(x)
  sd2 <- function(y){
    return(sd(y,na.rm = T))
  }
  if(is.matrix(x)) {
    SD<-apply(x,2,sd2)
  } else {
    SD <-sd2(x)
  }
  s2<-list(s1,SD)
  names(s2)<-c("summary","SD") # return summary and sd to each column
  return(s2)
}


print.summary <- function(x) { # x is a numerical vector or matrix
  avg.x = round(mean(x), 2)
  sd2 <- function(y){
    return(sd(y,na.rm = T))
  }
  if(is.matrix(x)) {
    SD <-round(apply(x,2,sd2), 2)
  } else {
    SD <- round(sd2(x), 2)
  }
  out = paste(avg.x, " (", SD, ")", sep = "")
  return(out)
}


# 2. function to sample treatment A
# input matrix.pi as a matrix of sampling probabilities, which could be non-normalized
A.sim <- function(matrix.pi) {
  N <- nrow(matrix.pi) # sample size
  K <- ncol(matrix.pi) # treatment options
  if (N<=1 | K<=1) stop("Sample size or treatment options are insufficient!")
  if (min(matrix.pi)<0) stop("Treatment probabilities should not be negative!")
  
  # normalize probabilities to add up to 1 and simulate treatment A for each row
  probs <- t(apply(matrix.pi,1,function(x){x/sum(x,na.rm = TRUE)}))
  A <- apply(probs, 1, function(x) sample(0:(K-1), 1, prob = x))
  return(A)
}
```

# Senario 2: 3 objectives

One stage, three treatments

Sample size N = 1000, 
Noise ~N(0,1)

```{r}
N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 5 # replication
w1 = 0.4
w2 = 0.1
w3 = 1 - w1 - w2

perc.TRL11 = perc.TRL12 = perc.TRL13 = perc.MOTRL10 = perc.MOTRL11 = perc.MOTRL12 = perc.MOTRL13 = rep(NA,iter)
EYs.TRL11.a = EYs.TRL12.a = EYs.TRL13.a = 
  EYs.TRL11.b = EYs.TRL12.b = EYs.TRL13.b = 
  EYs.TRL11.c = EYs.TRL12.c = EYs.TRL13.c = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = EYs.MOTRL10.c = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = EYs.MOTRL11.c = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = EYs.MOTRL12.c = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = EYs.MOTRL13.c = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-rnorm(N)              # each covariates follows N(0,1)
  x7<-rnorm(N)
  x8<-rnorm(N)
  x9<-rnorm(N)
  x10<-answer(N, x = c("No", "Yes"), name = "Smoke")
  
  X0<-cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1 + 0.05*x3)
  pi12 <- exp(0.5*x5 - 0.5*x1 + 0.5*x2)
  
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x2, x3,x4,x5))
  
  # g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  # change !
  g1.opt <- (x1 <= 0.5)*(x2 > -0.2) + (x1 > 0.5)*2*(1 - (x3 > -1)*(x3 < -0.5))
  
  # 3 models #
  ################# Objective 1 ####################
  # simulate stage 1 optimal g11.opt for reward1
  
  # 
  Y11 <- exp(1.67 + 0.2*x6 - abs(1.5*x7 + x4 - 1)*((A1 - g1.opt)^2)) - 
    3*(A1 == 1) + 2*(A1 == 2) + rnorm(N,0,1) # noise on A = 1 and 2
  
  ################# Objective 2 ####################
  # simulate stage 1 optimal g12.opt for reward2
  Y12 <- 0.42 + x5 + 0.5*x6 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 
    3*(exp((A1 == 0)) -1) - 1.8*(exp((A1 == 2)) -1) + rnorm(N,0,1) # noise on A = 1 and 2
  

  ############## Objective 3 : hidden ####################
  Y13 <- 1.1 + x4^2 + exp(0.2*x8) + 0.5*(A1 == 0)*(2*(g1.opt == 0) - 1) + 3*(A1 == 1)*(2*(g1.opt == 1) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) - 
    5*log((A1 == 0) + 1) + 2.5*log((A1 == 1) + 1) + rnorm(N,0,1) # noise on A = 1 and 2


  ############ Multi-Objective weighted-sum reward at stage 1 ##########
  # stage 1 outcome
  Ys1 = cbind(Y11, Y12, Y13)
  
  ################ Grow the trees ######################
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(Y11, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(Y12, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward3
  TRLtree13 <- TRL::DTRtree(Y13, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  w11 = c(w1, w2, w3)
  MOTRL10 = MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(i+10000)
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = rnorm(N2)
  x7.test = rnorm(N2)
  x8.test = rnorm(N2)
  x9.test = rnorm(N2)
  x10.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,
                  x6.test,x7.test,x8.test,x9.test,x10.test) # All of the covariates
  
  #new:
  # g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  g1.opt.test <- (x1.test <= 0.5)*(x2.test > -0.2) + (x1.test > 0.5)*2*(1 - (x3.test > -1)*(x3.test < -0.5))
  ####### stage 1 prediction #######
  # predict selection %
  g1.TRL11 = as.numeric(TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test)))
  g1.TRL12 = as.numeric(TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test)))
  g1.TRL13 = as.numeric(TRL::predict_DTR(TRLtree13,newdata=data.frame(X0.test)))
  
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  perc.TRL13[i] = 100 * mean(g1.opt.test == g1.TRL13)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL13))

                       # counterfactual mean outcome for TRL
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL11.a[i] = mean(exp(1.67 + 0.2*x6.test - abs(1.5*x7.test + x4.test - 1)*((g1.TRL11 - g1.opt.test)^2)) - 
                          3*(g1.TRL11 == 1) + 2*(g1.TRL11 == 2)+ rnorm(N2,0,1))
  EYs.TRL12.a[i] = mean(exp(1.67 + 0.2*x6.test - abs(1.5*x7.test + x4.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 
                          3*(g1.TRL12 == 1) + 2*(g1.TRL12 == 2)+ rnorm(N2,0,1))
  EYs.TRL13.a[i] = mean(exp(1.67 + 0.2*x6.test - abs(1.5*x7.test + x4.test - 1)*((g1.TRL13 - g1.opt.test)^2)) - 
                          3*(g1.TRL13 == 1) + 2*(g1.TRL13 == 2)+ rnorm(N2,0,1))
  ##### reward 2  # 2.5
  EYs.TRL11.b[i] = mean(0.42 + x5.test + 0.5*x6.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 3*(exp((g1.TRL11 == 0)) -1) - 1.8*(exp((g1.TRL11 == 2)) -1) + rnorm(N2,0,1))
  EYs.TRL12.b[i] = mean(0.42 + x5.test + 0.5*x6.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 3*(exp((g1.TRL12 == 0)) -1) - 1.8*(exp((g1.TRL12 == 2)) -1) + rnorm(N2,0,1))
  EYs.TRL13.b[i] = mean(0.42 + x5.test + 0.5*x6.test + 2*(g1.TRL13 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL13 == 2)*(2*(g1.opt.test ==2) - 1) + 3*(exp((g1.TRL13 == 0)) -1) - 1.8*(exp((g1.TRL13 == 2)) -1) + rnorm(N2,0,1))
  
  #### reward 3
  EYs.TRL11.c[i] = mean(1.1 + x4.test^2 + exp(0.2*x8.test) + 0.5*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) + 3*(g1.TRL11 == 1)*(2*(g1.opt.test == 1) - 1) + 1.5*(g1.TRL11 == 2)*(2*(g1.opt.test == 2) -1) - 5*log((g1.TRL11 == 0) + 1) + 2.5*log((g1.TRL11 == 1) + 1) + rnorm(N2,0,1))
  EYs.TRL12.c[i] = mean(1.1 + x4.test^2 + exp(0.2*x8.test) + 0.5*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) + 3*(g1.TRL12 == 1)*(2*(g1.opt.test == 1) - 1) + 1.5*(g1.TRL12 == 2)*(2*(g1.opt.test == 2) -1) - 5*log((g1.TRL12 == 0) + 1) + 2.5*log((g1.TRL12 == 1) + 1) + rnorm(N2,0,1))
  EYs.TRL13.c[i] = mean(1.1 + x4.test^2 + exp(0.2*x8.test) + 0.5*(g1.TRL13 == 0)*(2*(g1.opt.test == 0) - 1) + 3*(g1.TRL13 == 1)*(2*(g1.opt.test == 1) - 1) + 1.5*(g1.TRL13 == 2)*(2*(g1.opt.test == 2) -1) - 5*log((g1.TRL13 == 0) + 1) + 2.5*log((g1.TRL13 == 1) + 1) + rnorm(N2,0,1))
    
                                             # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL10.a[i] = mean(mapply(function(x, y, a, b, c) { exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + mean(2*(unlist(x) == 2)) + rnorm(N2,0,1)},
                                 g1.MOTRL10, g1.opt.test, x6.test, x7.test, x4.test))
  EYs.MOTRL11.a[i] = mean(mapply(function(x, y, a, b, c) { exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + mean(2*(unlist(x) == 2)) + rnorm(N2,0,1)},
                                 g1.MOTRL11, g1.opt.test, x6.test, x7.test, x4.test))
  EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b, c) { exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + mean(2*(unlist(x) == 2)) + rnorm(N2,0,1)},
                                 g1.MOTRL12, g1.opt.test, x6.test, x7.test, x4.test))
  EYs.MOTRL13.a[i] = mean(mapply(function(x, y, a, b, c) { exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + mean(2*(unlist(x) == 2)) + rnorm(N2,0,1)},
                                 g1.MOTRL13, g1.opt.test, x6.test, x7.test, x4.test))
  ##### reward 2
  EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b) 
    {0.42 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(3*(exp((unlist(x) == 0)) -1)) -
      mean(1.8*(exp((unlist(x) == 2)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL10, g1.opt.test, x5.test, x6.test))
  EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b) 
    {0.42 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(3*(exp((unlist(x) == 0)) -1)) -
      mean(1.8*(exp((unlist(x) == 2)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL11, g1.opt.test, x5.test, x6.test))
  EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b) 
    {0.42 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(3*(exp((unlist(x) == 0)) -1)) -
      mean(1.8*(exp((unlist(x) == 2)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL12, g1.opt.test, x5.test, x6.test))
  EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b) 
    {0.42 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(3*(exp((unlist(x) == 0)) -1)) -
      mean(1.8*(exp((unlist(x) == 2)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL13, g1.opt.test, x5.test, x6.test))
  
  #### reward 3
  EYs.MOTRL10.c[i] = mean(mapply(function(x, y, a, b) {
    1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  }, g1.MOTRL10, g1.opt.test, x4.test, x8.test))
  EYs.MOTRL11.c[i] =  mean(mapply(function(x, y, a, b) {
    1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  }, g1.MOTRL11, g1.opt.test, x4.test, x8.test))
  EYs.MOTRL12.c[i] = mean(mapply(function(x, y, a, b) {
    1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  }, g1.MOTRL12, g1.opt.test, x4.test, x8.test))
  EYs.MOTRL13.c[i] = mean(mapply(function(x, y, a, b) {
    1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  }, g1.MOTRL13, g1.opt.test, x4.test, x8.test))

}

Result = data.frame(matrix(NA, ncol = 4, nrow = 7))
names(Result) = c("perc.opt", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}", "mE{Y3*(g_hat)}")

Result$perc.opt = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.TRL13), print.summary(perc.MOTRL10), 
                    print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.TRL13.a), print.summary(EYs.MOTRL10.a), 
                            print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))

Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.TRL13.b), print.summary(EYs.MOTRL10.b), 
                            print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))

Result$`mE{Y3*(g_hat)}` = c(print.summary(EYs.TRL11.c), print.summary(EYs.TRL12.c), print.summary(EYs.TRL13.c), print.summary(EYs.MOTRL10.c), 
                            print.summary(EYs.MOTRL11.c), print.summary(EYs.MOTRL12.c), print.summary(EYs.MOTRL13.c))

Result
```

try on Aug 5: Good!
```{r}
N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 10 # replication
w1 = 0.35
w2 = 0.35
w3 = 1 - w1 - w2

perc.TRL11 = perc.TRL12 = perc.TRL13 = perc.MOTRL10 = perc.MOTRL11 = perc.MOTRL12 = perc.MOTRL13 = rep(NA,iter)
EYs.TRL11.a = EYs.TRL12.a = EYs.TRL13.a = 
  EYs.TRL11.b = EYs.TRL12.b = EYs.TRL13.b = 
  EYs.TRL11.c = EYs.TRL12.c = EYs.TRL13.c = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = EYs.MOTRL10.c = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = EYs.MOTRL11.c = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = EYs.MOTRL12.c = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = EYs.MOTRL13.c = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-rnorm(N)              # each covariates follows N(0,1)
  x7<-rnorm(N)
  x8<-rnorm(N)
  x9<-rnorm(N)
  x10<-answer(N, x = c("No", "Yes"), name = "Smoke")
  
  X0<-cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1 + 0.05*x3)
  pi12 <- exp(0.5*x5 - 0.5*x1 + 0.5*x2)
  
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x2, x3,x4,x5))
  
  # g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  # change !
  g1.opt <- (x1 <= 0.5)*(x2 > -0.2) + (x1 > 0.5)*2*(1 - (x3 > -1)*(x3 < -0.5))
  
  # 3 models #
  ################# Objective 1 ####################
  # simulate stage 1 optimal g11.opt for reward1
  
  # 
  Y11 <- 0.57 + exp(1.67 + 0.2*x6 - abs(1.5*x7 + x4 - 1)*((A1 - g1.opt)^2)) - 
    3*(A1 == 1) + rnorm(N,0,1) # noise on A = 1 and 2
  ################# Objective 2 ####################
  # simulate stage 1 optimal g12.opt for reward2
  Y12 <- 1.9 + x5 + 0.5*x6 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 0.5*(A1 == 1)*(2*(g1.opt == 1) -1) +
    1.8*(exp((A1 == 1)) -1) + rnorm(N,0,1) # noise on A = 1 and 2
  ############## Objective 3 : hidden ####################
  Y13 <- 5.32 + x8 - exp(0.1 + 2*(x10 == "No"))*(1*(A1 == 1) + 0.5*(A1 == 0) + 0.1*(A1 == 2)) + rnorm(N,0,1) # noise on A = 1 and 2
  
  # minY11 = min(Y11); maxY11 = max(Y11); rangeY11 = maxY11 - minY11
  # minY12 = min(Y12); maxY12 = max(Y12); rangeY12 = maxY12 - minY12
  # minY13 = min(Y13); maxY13 = max(Y13); rangeY13 = maxY13 - minY13

  Y11 = scales::rescale(Y11, to = c(0,5))
  Y12 = scales::rescale(Y12, to = c(0,5))
  Y13 = scales::rescale(Y13, to = c(0,5))
  
  ############ Multi-Objective weighted-sum reward at stage 1 ##########
  # stage 1 outcome
  Ys1 = cbind(Y11, Y12, Y13)
  
  ################ Grow the trees ######################
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(Y11, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(Y12, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward3
  TRLtree13 <- TRL::DTRtree(Y13, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  w11 = c(w1, w2, w3)
  MOTRL10 = MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=20,depth = 4)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(i+10000)
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = rnorm(N2)
  x7.test = rnorm(N2)
  x8.test = rnorm(N2)
  x9.test = rnorm(N2)
  x10.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,
                  x6.test,x7.test,x8.test,x9.test,x10.test) # All of the covariates
  
  #new:
  # g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  g1.opt.test <- (x1.test <= 0.5)*(x2.test > -0.2) + (x1.test > 0.5)*2*(1 - (x3.test > -1)*(x3.test < -0.5))
  ####### stage 1 prediction #######
  # predict selection %
  g1.TRL11 = as.numeric(TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test)))
  g1.TRL12 = as.numeric(TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test)))
  g1.TRL13 = as.numeric(TRL::predict_DTR(TRLtree13,newdata=data.frame(X0.test)))
  
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  perc.TRL13[i] = 100 * mean(g1.opt.test == g1.TRL13)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL13))

                       # counterfactual mean outcome for TRL
  
  
  
   
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL11.a[i] = mean(0.57 + exp(1.67 + 0.2*x6.test - abs(1.5*x7.test + x4.test - 1)*((g1.TRL11 - g1.opt.test)^2)) -
                          3*(g1.TRL11 == 1) + rnorm(N2,0,1)) # - minY11) * 5 / rangeY11
  EYs.TRL12.a[i] = mean(0.57 + exp(1.67 + 0.2*x6.test - abs(1.5*x7.test + x4.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 
                          3*(g1.TRL12 == 1) + rnorm(N2,0,1)) # - minY11) * 5 / rangeY11
  EYs.TRL13.a[i] = mean(0.57 + exp(1.67 + 0.2*x6.test - abs(1.5*x7.test + x4.test - 1)*((g1.TRL13 - g1.opt.test)^2)) - 
                          3*(g1.TRL13 == 1) + rnorm(N2,0,1)) # - minY11) * 5 / rangeY11
  ##### reward 2  # 2.5
  EYs.TRL11.b[i] = mean(1.9 + x5.test + 0.5*x6.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
                           1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 0.5*(g1.TRL11 == 1)*(2*(g1.opt.test ==1) - 1) +
                           1.8*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,1))
                    #- minY12) * 5 / rangeY12
  EYs.TRL12.b[i] = mean(1.9 + x5.test + 0.5*x6.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
                           1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 0.5*(g1.TRL12 == 1)*(2*(g1.opt.test ==1) - 1) +
                           1.8*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,1))
                    #- minY12) * 5 / rangeY12
  EYs.TRL13.b[i] = mean(1.9 + x5.test + 0.5*x6.test + 2*(g1.TRL13 == 0)*(2*(g1.opt.test == 0) - 1) +
                           1.5*(g1.TRL13 == 2)*(2*(g1.opt.test ==2) - 1) + 0.5*(g1.TRL13 == 1)*(2*(g1.opt.test ==1) - 1) +
                           1.8*(exp((g1.TRL13 == 1)) -1) + rnorm(N2,0,1))
                    #- minY12) * 5 / rangeY12
  
  #### reward 3
  # EYs.TRL11.c[i] = (mean(1.1 + x4.test^2 + exp(0.2*x8.test) + 0.5*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) + 3*(g1.TRL11 == 1)*(2*(g1.opt.test == 1) - 1) + 1.5*(g1.TRL11 == 2)*(2*(g1.opt.test == 2) -1) - 5*log((g1.TRL11 == 0) + 1) + 2.5*log((g1.TRL11 == 1) + 1) + rnorm(N2,0,1)) - minY13) * 5 / rangeY13
  # EYs.TRL12.c[i] = (mean(1.1 + x4.test^2 + exp(0.2*x8.test) + 0.5*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) + 3*(g1.TRL12 == 1)*(2*(g1.opt.test == 1) - 1) + 1.5*(g1.TRL12 == 2)*(2*(g1.opt.test == 2) -1) - 5*log((g1.TRL12 == 0) + 1) + 2.5*log((g1.TRL12 == 1) + 1) + rnorm(N2,0,1)) - minY13) * 5 / rangeY13
  # EYs.TRL13.c[i] = (mean(1.1 + x4.test^2 + exp(0.2*x8.test) + 0.5*(g1.TRL13 == 0)*(2*(g1.opt.test == 0) - 1) + 3*(g1.TRL13 == 1)*(2*(g1.opt.test == 1) - 1) + 1.5*(g1.TRL13 == 2)*(2*(g1.opt.test == 2) -1) - 5*log((g1.TRL13 == 0) + 1) + 2.5*log((g1.TRL13 == 1) + 1) + rnorm(N2,0,1)) - minY13) * 5 / rangeY13
  
  EYs.TRL11.c[i] = mean(5.32 + x8.test - exp(0.1 + 2*(x10.test == "No"))*(1*(g1.TRL11 == 1) + 0.5*(g1.TRL11 == 0) + 0.1*(g1.TRL11 == 2)) + rnorm(N,0,1))
                   # - minY13) * 5 / rangeY13
  EYs.TRL12.c[i] = mean(5.32 + x8.test - exp(0.1 + 2*(x10.test == "No"))*(1*(g1.TRL12 == 1) + 0.5*(g1.TRL12 == 0) + 0.1*(g1.TRL12 == 2)) + rnorm(N,0,1))
                   # - minY13) * 5 / rangeY13
  EYs.TRL13.c[i] = mean(5.32 + x8.test - exp(0.1 + 2*(x10.test == "No"))*(1*(g1.TRL13 == 1) + 0.5*(g1.TRL13 == 0) + 0.1*(g1.TRL13 == 2)) + rnorm(N,0,1))
                   # - minY13) * 5 / rangeY13
                
  
                                             # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL10.a[i] = mean(mapply(function(x, y, a, b, c) {0.57 + exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL10, g1.opt.test, x6.test, x7.test, x4.test)) # - minY11) * 5 / rangeY11
  EYs.MOTRL11.a[i] = mean(mapply(function(x, y, a, b, c) {0.57 + exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL11, g1.opt.test, x6.test, x7.test, x4.test)) # - minY11) * 5 / rangeY11
  EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b, c) {0.57 + exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL12, g1.opt.test, x6.test, x7.test, x4.test)) # - minY11) * 5 / rangeY11
  EYs.MOTRL13.a[i] = mean(mapply(function(x, y, a, b, c) {0.57 + exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
      mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL13, g1.opt.test, x6.test, x7.test, x4.test)) # - minY11) * 5 / rangeY11
  ##### reward 2
  EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b) 
    {1.9 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(0.5*(unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(1.8*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, g1.MOTRL10, g1.opt.test, x5.test, x6.test)) # - minY12) * 5 / rangeY12
  EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b) 
    {1.9 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(0.5*(unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(1.8*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, g1.MOTRL11, g1.opt.test, x5.test, x6.test)) # - minY12) * 5 / rangeY12
  EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b) 
    {1.9 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(0.5*(unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(1.8*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, g1.MOTRL12, g1.opt.test, x5.test, x6.test)) # - minY12) * 5 / rangeY12
  EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b) 
    {1.9 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(0.5*(unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(1.8*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, g1.MOTRL13, g1.opt.test, x5.test, x6.test)) # - minY12) * 5 / rangeY12
  
  #### reward 3
  # EYs.MOTRL10.c[i] = (mean(mapply(function(x, y, a, b) {
  #   1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  # }, g1.MOTRL10, g1.opt.test, x4.test, x8.test)) - minY13) * 5 / rangeY13
  # EYs.MOTRL11.c[i] =  (mean(mapply(function(x, y, a, b) {
  #   1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  # }, g1.MOTRL11, g1.opt.test, x4.test, x8.test)) - minY13) * 5 / rangeY13
  # EYs.MOTRL12.c[i] = (mean(mapply(function(x, y, a, b) {
  #   1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  # }, g1.MOTRL12, g1.opt.test, x4.test, x8.test)) - minY13) * 5 / rangeY13
  # EYs.MOTRL13.c[i] = (mean(mapply(function(x, y, a, b) {
  #   1.1 + a^2 + exp(0.2*b) + mean(0.5*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(3*(unlist(x) == 1)*(2*(y == 1) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y == 2) -1)) - mean(5*log((unlist(x) == 0) + 1)) + mean(2.5*log((unlist(x) == 1) + 1)) + rnorm(N2,0,1) # noise on A = 1 and 2
  # }, g1.MOTRL13, g1.opt.test, x4.test, x8.test)) - minY13) * 5 / rangeY13
  
  EYs.MOTRL10.c[i] = mean(mapply(function(x, y, a, b) {
    5.32 + a - exp(0.1 + 2*(b == "No"))*(1*(unlist(x) == 1) + 0.5*(unlist(x) == 0) + 0.1*(unlist(x) == 2)) + rnorm(N2,0,1) # noise on A = 1 and 2
    }, g1.MOTRL10, g1.opt.test, x8.test, x10.test)) # - minY13) * 5 / rangeY13
  EYs.MOTRL11.c[i] = mean(mapply(function(x, y, a, b) {
    5.32 + a - exp(0.1 + 2*(b == "No"))*(1*(unlist(x) == 1) + 0.5*(unlist(x) == 0) + 0.1*(unlist(x) == 2)) + rnorm(N2,0,1) # noise on A = 1 and 2
    }, g1.MOTRL11, g1.opt.test, x8.test, x10.test)) # - minY13) * 5 / rangeY13
  EYs.MOTRL12.c[i] = mean(mapply(function(x, y, a, b) {
    5.32 + a - exp(0.1 + 2*(b == "No"))*(1*(unlist(x) == 1) + 0.5*(unlist(x) == 0) + 0.1*(unlist(x) == 2)) + rnorm(N2,0,1) # noise on A = 1 and 2
    }, g1.MOTRL12, g1.opt.test, x8.test, x10.test)) # - minY13) * 5 / rangeY13
  EYs.MOTRL13.c[i] = mean(mapply(function(x, y, a, b) {
    5.32 + a - exp(0.1 + 2*(b == "No"))*(1*(unlist(x) == 1) + 0.5*(unlist(x) == 0) + 0.1*(unlist(x) == 2)) + rnorm(N2,0,1) # noise on A = 1 and 2
    }, g1.MOTRL13, g1.opt.test, x8.test, x10.test)) # - minY13) * 5 / rangeY13
}

Result = data.frame(matrix(NA, ncol = 4, nrow = 7))
names(Result) = c("perc.opt", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}", "mE{Y3*(g_hat)}")

Result$perc.opt = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.TRL13), print.summary(perc.MOTRL10), 
                    print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.TRL13.a), print.summary(EYs.MOTRL10.a), 
                            print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))

Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.TRL13.b), print.summary(EYs.MOTRL10.b), 
                            print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))

Result$`mE{Y3*(g_hat)}` = c(print.summary(EYs.TRL11.c), print.summary(EYs.TRL12.c), print.summary(EYs.TRL13.c), print.summary(EYs.MOTRL10.c), 
                            print.summary(EYs.MOTRL11.c), print.summary(EYs.MOTRL12.c), print.summary(EYs.MOTRL13.c))

Result
```

Output for Figure 3D
```{r}
weights = weights3D[,1:3]
# weights = weights3D[1:10,1:3]

N<-1000 # sample size of training data
N2<-1000 # sample size of testing data
iter <- 100 # replication

# Result = data.frame(matrix(NA, ncol = 5, nrow = 231))
# names(Result) = c("perc.opt.mean", "perc.opt.sd", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}", "mE{Y3*(g_hat)}")

for (row in (140:nrow(weights))) {
  perc.MOTRL12 = EYs.MOTRL12.a = EYs.MOTRL12.b = EYs.MOTRL12.c = rep(NA,iter) # estimated mean counterfactual outcome
  
  for (i in 1:iter) {
    # Simulation begin
    set.seed(i+300)
    x1<-rnorm(N)              # each covariates follows N(0,1)
    x2<-rnorm(N)
    x3<-rnorm(N)
    x4<-rnorm(N)
    x5<-rnorm(N)
    x6<-rnorm(N)              # each covariates follows N(0,1)
    x7<-rnorm(N)
    x8<-rnorm(N)
    x9<-rnorm(N)
    x10<-answer(N, x = c("No", "Yes"), name = "Smoke")
    
    X0<-cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10) # All of the covariates
  
    ############### stage 1 data simulation ##############
    # simulate A1, true stage 1 treatment with K1=3
    pi10 <- rep(1, N)
    pi11 <- exp(0.5*x4 + 0.5*x1 + 0.05*x3)
    pi12 <- exp(0.5*x5 - 0.5*x1 + 0.5*x2)
    
    # weights matrix
    matrix.pi1 <- cbind(pi10, pi11, pi12)
    A1 <- A.sim(matrix.pi1)
    class.A1 <- sort(unique(A1))
    # propensity stage 1
    pis1.hat <- M.propen(A1, cbind(x1,x2, x3,x4,x5))
    
    # g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
    g1.opt <- (x1 <= 0.5)*(x2 > -0.2) + (x1 > 0.5)*2*(1 - (x3 > -1)*(x3 < -0.5))
    
    # 3 models #
    ################# Objective 1 ####################
    # simulate stage 1 optimal g11.opt for reward1
    Y11 <- 0.57 + exp(1.67 + 0.2*x6 - abs(1.5*x7 + x4 - 1)*((A1 - g1.opt)^2)) - 
      3*(A1 == 1) + rnorm(N,0,1) # noise on A = 1 and 2
    ################# Objective 2 ####################
    # simulate stage 1 optimal g12.opt for reward2
    Y12 <- 1.9 + x5 + 0.5*x6 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 0.5*(A1 == 1)*(2*(g1.opt == 1) -1) +
      1.8*(exp((A1 == 1)) -1) + rnorm(N,0,1) # noise on A = 1 and 2
    ############## Objective 3 : hidden ####################
    Y13 <- 5.32 + x8 - exp(0.1 + 2*(x10 == "No"))*(1*(A1 == 1) + 0.5*(A1 == 0) + 0.1*(A1 == 2)) + rnorm(N,0,1) # noise on A = 1 and 2
    
    Y11 = scales::rescale(Y11, to = c(0,5))
    Y12 = scales::rescale(Y12, to = c(0,5))
    Y13 = scales::rescale(Y13, to = c(0,5))
    
    ############ Multi-Objective weighted-sum reward at stage 1 ##########
    # stage 1 outcome
    Ys1 = cbind(Y11, Y12, Y13)
    
    # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
    w11 = as.numeric(weights[row,])
    MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.02, minsplit=35, depth = 4)
    MOTRLtree12 = MOTRL12$tree
    
    ############################################
    # prediction using new data
    ############################################
    set.seed(i+10000)
    x1.test = rnorm(N2)
    x2.test = rnorm(N2)
    x3.test = rnorm(N2)
    x4.test = rnorm(N2)
    x5.test = rnorm(N2)
    x6.test = rnorm(N2)
    x7.test = rnorm(N2)
    x8.test = rnorm(N2)
    x9.test = rnorm(N2)
    x10.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
    
    X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,
                    x6.test,x7.test,x8.test,x9.test,x10.test) # All of the covariates
    
    g1.opt.test <- (x1.test <= 0.5)*(x2.test > -0.2) + (x1.test > 0.5)*2*(1 - (x3.test > -1)*(x3.test < -0.5))
    # predict selection %
    g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
   
    
    # percentage of true prediction
    perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% unlist(y), g1.opt.test, g1.MOTRL12))
    
    # counterfactual mean outcome for MOTRL
    ##### reward 1
    EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b, c) {0.57 + exp(1.67 + 0.2*a - abs(1.5*b + c - 1)*mean((unlist(x) - y)^2)) - 
        mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                   g1.MOTRL12, g1.opt.test, x6.test, x7.test, x4.test)) # - minY11) * 5 / rangeY11
    ##### reward 2
    EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b) {1.9 + a + 0.5*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(0.5*(unlist(x) == 1)*(2*(y ==1) -1)) + 
        mean(1.8*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, g1.MOTRL12, g1.opt.test, x5.test, x6.test)) # - minY12) * 5 / rangeY12
    #### reward 3
    EYs.MOTRL12.c[i] = mean(mapply(function(x, y, a, b) {
      5.32 + a - exp(0.1 + 2*(b == "No"))*(1*(unlist(x) == 1) + 0.5*(unlist(x) == 0) + 0.1*(unlist(x) == 2)) + rnorm(N2,0,1) # noise on A = 1 and 2
      }, g1.MOTRL12, g1.opt.test, x8.test, x10.test)) # - minY13) * 5 / rangeY13
  }
  Result[row,] = c(mean(perc.MOTRL12), sd(perc.MOTRL12), mean(EYs.MOTRL12.a), mean(EYs.MOTRL12.b), mean(EYs.MOTRL12.c))
  weights3D[row,4:8] = Result[row,]
}

```



Figure 4: For Scenario 2, n = 1000, e ~ N(0,1)

(a). 3D plot for three objectives
```{r}
Figure4Data = weights3D

x = Figure4Data$EY1
y = Figure4Data$EY2
z = Figure4Data$EY3


```

(b). 3D plot for opt%

```{r}
Figure4Data = weights3D
tmp <- as.data.frame(Figure4Data[,c(1,2,4)])
x = seq(0, 100, by=5)*1.5
y = seq(0, 100, by=5)*1.5
z = as.matrix(pivot_wider(tmp, names_from = w1, values_from = opt.mean)[,-1])
image2D(z, border = "black") 

hist3D(z = z, scale = FALSE, expand = 0.007, facets = T, lwd = 0.8, 
       theta = -55,  phi = 20,
       contour = list(side = c("zmin", "z", "160")), 
       zlim = c(10, 160), image = list(side = 160),
       alpha = 0.6, border = "black",shade = 0,  ltheta = 50)


hist3D(z = z, scale = FALSE, expand = 0.006, facets = T, lwd = 0.8, 
       theta = -55,  phi = 20,
       contour = list(side = c("zmin", "z", "140")), 
       zlim = c(10, 140), image = list(side = 140),
       alpha = 0.6, border = "black",shade = 0,  ltheta = 50)

```



```{r}
Figure4Data = weights3D
tmp <- as.data.frame(Figure4Data[,c(1,2,4)])
x = seq(0, 100, by=5)
y = seq(0, 100, by=5)
z = as.matrix(pivot_wider(tmp, names_from = w1, values_from = opt.mean)[,-1])
z[is.na(z)] <- 0

zlim <- range(z)
zlen <- zlim[2] - zlim[1] + 1
colorlut <- terrain.colors(zlen) # height color lookup table
col <- colorlut[ trunc(z - zlim[1] + 1) ] # assign colors to heights for each point
open3d()
surface3d(x, y, z, color = col, back = "lines")

persp3D(z = z)

persp3D(z = Volcano, contour = list(side = c("zmin", "z", "350")), 
       zlim = c(-100, 400), phi = 20, image = list(side = 350))
```




plot3D::surf3D
```{r}
R <- 3; r <- 2
M <- mesh(seq(0, 2*pi,length.out=50), seq(0, pi,length.out=50))

alpha <- M$x; beta <- M$y

x <- (R + r*cos(alpha)) * cos(beta)
y <- (R + r*cos(alpha)) * sin(beta)
z <-  r * sin(alpha)

surf3D(x = x, y = y, z = z, colkey=TRUE, bty="b2",
       phi = 40, theta = 30, main="Tri-objective Optimization by MOTRL with 70% tolerance")
```


surface3d
```{r}
z <- 2 * volcano        # Exaggerate the relief
x <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)
y <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)

zlim <- range(z)
zlen <- zlim[2] - zlim[1] + 1

colorlut <- terrain.colors(zlen) # height color lookup table
col <- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point
open3d()
surface3d(x, y, z, color = col, back = "lines")
```



persp
```{r}
cone <- function(x, y) {
  sqrt(x ^ 2 + y ^ 2)
  }
# prepare variables.
x <- y <- seq(-1, 1, length = 30)
z <- outer(x, y, cone)
# plot the 3D surface
persp(x, y, z)

# plot the 3D surface
# Adding Titles and Labeling Axes to Plot
persp(x, y, z,
main="Perspective Plot of a Cone",
zlab = "Height",
theta = 30, phi = 15,
col = "orange", shade = 0.4)


# Visualizing a simple DEM model
z <- 2 * volcano     # Exaggerate the relief
x <- 10 * (1:nrow(z)) # 10 meter spacing (S to N)
y <- 10 * (1:ncol(z)) # 10 meter spacing (E to W)
# Don't draw the grid lines : border = NA
persp(x, y, z, theta = 135, phi = 30,
      col = "brown", scale = FALSE,
    ltheta = -120, shade = 0.75,
      border = NA, box = FALSE)
```


lattice
```{r}
# begin generating my 3D shape
b <- seq(from=0, to=20,by=0.5)
s <- seq(from=0, to=20,by=0.5)
payoff <- expand.grid(b=b,s=s)
payoff$payoff <- payoff$b - payoff$s # z = b - s
payoff$payoff[payoff$payoff < -1] <- -1
# end generating my 3D shape


wireframe(payoff ~ s * b, payoff, shade = TRUE, aspect = c(1, 1),
    light.source = c(10,10,10), main = "Scenario 2: 3 objective",
    scales = list(z.ticks=5,arrows=FALSE, col="black", font=10, tck=0.5),
    screen = list(z = -70, x = -90, y = -30))

```



```{r}
## volcano  ## 87 x 61 matrix
wireframe(volcano, shade = TRUE,
          aspect = c(61/87, 0.4),
          light.source = c(10,0,10))


# 
g <- expand.grid(x = 1:10, y = 5:15, gr = 1:2)
g$z <- log((g$x^g$gr + g$y^2) * g$gr)
wireframe(z ~ x * y, data = g, groups = gr,
          scales = list(arrows = FALSE),
          drape = TRUE, colorkey = TRUE,
          screen = list(z = 30, x = -60))


# z ~ x * y | ?
cloud(Sepal.Length ~ Petal.Length * Petal.Width | Species, data = iris,
      screen = list(x = -90, y = 70), distance = .4, zoom = .6)

## cloud.table
cloud(prop.table(Titanic, margin = 1:3),
      type = c("p", "h"), strip = strip.custom(strip.names = TRUE),
      scales = list(arrows = FALSE, distance = 2), panel.aspect = 0.7,
      zlab = "Proportion")[, 1]

## transparent axes
par.set <-
    list(axis.line = list(col = "transparent"),
         clip = list(panel = "off"))
print(cloud(Sepal.Length ~ Petal.Length * Petal.Width, 
            data = iris, cex = .8, 
            groups = Species, 
            main = "Stereo",
            screen = list(z = 20, x = -70, y = 3),
            par.settings = par.set,
            scales = list(col = "black")),
      split = c(1,1,2,1), more = TRUE)

print(cloud(Sepal.Length ~ Petal.Length * Petal.Width,
            data = iris, cex = .8, 
            groups = Species,
            main = "Stereo",
            screen = list(z = 20, x = -70, y = 0),
            par.settings = par.set,
            scales = list(col = "black")),
      split = c(2,1,2,1))


```



```{r}
library(plotly)
kd <- with(MASS::geyser, MASS::kde2d(duration, waiting, n = 50))
fig <- plot_ly(x = kd$x, y = kd$y, z = kd$z) %>% add_surface()
fig



df = weights3D[,6:8]
myMatrix = data.frame2matrix(df, 'EY1', 'EY2', 'EY3')
myMatrix
plot_ly(x = weights3D$EY1, y = weights3D$EY2, z = myMatrix) %>% add_surface()




library(tidyr)

tmp <- as.data.frame(weights2D1stage[3:19,c(4, 6, 2)])
spread(tmp, EY2, opt)
tmp$opt = tmp$opt/20

```


GPareto
```{r}
## 3D plots example
library(GPareto)
library(rgl)
set.seed(5)
X <- matrix(runif(60), ncol=3)
Xnd <- t(nondominated_points(t(X))) # This is the solution set
plot3d(X)
plot3d(Xnd, col="red", size=8, add=TRUE)
plot3d(x = min(Xnd[,1]), 
       y = min(Xnd[,2]), 
       z = min(Xnd[,3]), 
       col="green", size=8, add=TRUE)

X.range <- diff(apply(X,2,range))
bounds <- rbind(apply(X,2,min) - 0.1*X.range, apply(X,2,max) + 0.1*X.range)


plotParetoEmp(nondominatedPoints = Xnd, add=TRUE, bounds=bounds, alpha=0.8,# max = TRUE,
              col = "red")
```


```{r}
X = weights3D[,6:8]
# finding non-dominated point
Xnd <- -t(nondominated_points(t(-X))) # This is the solution set
plot3d(X)
plot3d(Xnd, col="red", size=8, add=TRUE)
plot3d(x = min(Xnd[,1]), 
       y = min(Xnd[,2]), 
       z = min(Xnd[,3]), 
       col="green", size=8, add=TRUE)
X.range <- diff(apply(X,2,range))
bounds <- rbind(apply(X,2,min) - 0.1*X.range, apply(X,2,max) + 0.1*X.range)

save(Xn, file = "plot3D_new.")



library(xlsx)
# Write the first data set in a new workbook
write.xlsx(Xnd, "3Dplot_new.xlsx")
# plotParetoEmp(nondominatedPoints = Xnd, add=TRUE, bounds=bounds, alpha=0.8,# max = TRUE,
#               col = "red")


# finding non-dominated point
# library(ecr)
# which.dominated(as.matrix(X))
```


```{r}

## With maximization
x <- c(0.2, 0.4, 0.6, 0.8)
y <- c(0.8, 0.7, 0.5, 0.1)
plot(x, y, col = "green", pch = 20) 
plotParetoEmp(cbind(x, y), col = "green")
## Alternative
plotParetoEmp(cbind(x, y), col = "red", add = FALSE)
## With maximization
plotParetoEmp(cbind(x, y), col = "blue", max = TRUE)
```

```{r}
library(graphics)
persp(x = df$EY1, y = df$EY2, z = df$opt)
```


