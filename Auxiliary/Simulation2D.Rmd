---
title: "MOTRL Simulation"
author: "Yao Song"
date: "7/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(randomForest)
library(wakefield)
library(dplyr)
library(psych)
library(psychTools)
library(readr)
library(scales)
library(ggplot2)
```

# Functions for simulation.

```{r}
# 1. function to summarize simulation results
summary2 <- function(x) { # x is a numerical vector or matrix
  s1 <- summary(x)
  sd2 <- function(y){
    return(sd(y,na.rm = T))
  }
  if(is.matrix(x)) {
    SD<-apply(x,2,sd2)
  } else {
    SD <-sd2(x)
  }
  s2<-list(s1,SD)
  names(s2)<-c("summary","SD") # return summary and sd to each column
  return(s2)
}


print.summary <- function(x) { # x is a numerical vector or matrix
  avg.x = round(mean(x), 2)
  sd2 <- function(y){
    return(sd(y,na.rm = T))
  }
  if(is.matrix(x)) {
    SD <-round(apply(x,2,sd2), 2)
  } else {
    SD <- round(sd2(x), 2)
  }
  out = paste(avg.x, " (", SD, ")", sep = "")
  return(out)
}


# 2. function to sample treatment A
# input matrix.pi as a matrix of sampling probabilities, which could be non-normalized
A.sim <- function(matrix.pi) {
  N <- nrow(matrix.pi) # sample size
  K <- ncol(matrix.pi) # treatment options
  if (N<=1 | K<=1) stop("Sample size or treatment options are insufficient!")
  if (min(matrix.pi)<0) stop("Treatment probabilities should not be negative!")
  
  # normalize probabilities to add up to 1 and simulate treatment A for each row
  probs <- t(apply(matrix.pi,1,function(x){x/sum(x,na.rm = TRUE)}))
  A <- apply(probs, 1, function(x) sample(0:(K-1), 1, prob = x))
  return(A)
}
```

# Senario 1 

## (A). One stage, three treatments

(i) Small sample size (N = 500), small noice (~N(0,1))
(ii) Large sample size (N = 1000), small noice (~N(0,1))

```{r}
N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 100 # replication
w0 = 0.8


perc.TRL11 = perc.TRL12 = perc.MOTRL10 = perc.MOTRL11 = perc.MOTRL12 = perc.MOTRL13 = rep(NA,iter)
EYs.TRL11.a = EYs.TRL12.a = EYs.TRL11.b = EYs.TRL12.b = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
  X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1)
  pi12 <- exp(0.5*x5 - 0.5*x1)
  
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
  
  g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  
  # 3 models #
  ################# Objective 1 ####################
  # simulate stage 1 optimal g11.opt for reward1
  Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
 
  ################# Objective 2 ####################
  # simulate stage 1 optimal g12.opt for reward2
  Y12 <- 2.37 + x4 + 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 1.5*(exp((A1 == 1)) -1) + rnorm(N,0,1)
  
  # Y11 = rescale(Y11, to = c(0,5))
  # Y12 = rescale(Y12, to = c(0,5))

  ############ Multi-Objective weighted-sum reward at stage 1 ##########
  # stage 1 outcome
  Ys1 = cbind(Y11, Y12)
  
  ################ Grow the trees ######################
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(Y11, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(Y12, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  
  w11 = c(w0, 1-w0)
  MOTRL10 = MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(i+10000)
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
  
  #new:
  g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  
  ####### stage 1 prediction #######
  # predict selection %
  g1.TRL11 = TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test))
  g1.TRL12 = TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test))
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL13))

  
                                        # counterfactual mean outcome for TRL
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL11.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL11 - g1.opt.test)^2)) - 3*(g1.TRL11 == 1) + rnorm(N2,0,1))
  EYs.TRL12.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 3*(g1.TRL12 == 1) + rnorm(N2,0,1))
  
  ##### reward 2  # 2.5
  EYs.TRL11.b[i] = mean(2.36 + x4.test + 2*x5.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,1))
  EYs.TRL12.b[i] = mean(2.36 + x4.test + 2*x5.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,1))
  
   
                                             # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL10.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL10, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL11.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL11, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL12, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL13.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL13, g1.opt.test, x4.test, x1.test))


  ##### reward 2
  EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL10, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)},   
    g1.MOTRL11, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL12, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL13, g1.opt.test, x4.test, x5.test, x6.test))
}

Result = data.frame(matrix(NA, ncol = 3, nrow = 6))
names(Result) = c("perc.opt", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}")

Result$perc.opt = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.MOTRL10), print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.MOTRL10.a), print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))

Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.MOTRL10.b), print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))

Result
```


Try AIPW outputs: No
```{r}
N<-500 # sample size of training data
N2<-1000 # sample size of test data
iter <- 100 # replication
w0 = 0.3


perc.TRL11 = perc.TRL12 = perc.MOTRL10 = perc.MOTRL11 = perc.MOTRL12 = perc.MOTRL13 = rep(NA,iter)
EYs.TRL11.a = EYs.TRL12.a = EYs.TRL11.b = EYs.TRL12.b = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
  X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1)
  pi12 <- exp(0.5*x5 - 0.5*x1)
  
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
  
  g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  
  # 3 models #
  ################# Objective 1 ####################
  # simulate stage 1 optimal g11.opt for reward1
  Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
 
  ################# Objective 2 ####################
  # simulate stage 1 optimal g12.opt for reward2
  Y12 <- 2.37 + x4 + 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 1.5*(exp((A1 == 1)) -1) + rnorm(N,0,1)
  
  # Y11 = rescale(Y11, to = c(0,5))
  # Y12 = rescale(Y12, to = c(0,5))

  ############ Multi-Objective weighted-sum reward at stage 1 ##########
  # stage 1 outcome
  Ys1 = cbind(Y11, Y12)
  
  ################ Grow the trees ######################
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(Y11, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(Y12, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  
  w11 = c(w0, 1-w0)
  MOTRL10 = MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(i+10000)
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
  
  #new:
  g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  
  ####### stage 1 prediction #######
  # predict selection %
  g1.TRL11 = TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test))
  g1.TRL12 = TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test))
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL13))

  
                                        # counterfactual mean outcome for TRL
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL11.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL11 - g1.opt.test)^2)) - 3*(g1.TRL11 == 1) + rnorm(N2,0,1))
  EYs.TRL12.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 3*(g1.TRL12 == 1) + rnorm(N2,0,1))
  
  ##### reward 2  # 2.5
  EYs.TRL11.b[i] = mean(2.36 + x4.test + 2*x5.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,1))
  EYs.TRL12.b[i] = mean(2.36 + x4.test + 2*x5.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,1))
  
   
                                             # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL10.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL10, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL11.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL11, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL12, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL13.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL13, g1.opt.test, x4.test, x1.test))


  ##### reward 2
  EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL10, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)},   
    g1.MOTRL11, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL12, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 + a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL13, g1.opt.test, x4.test, x5.test, x6.test))
}

Result = data.frame(matrix(NA, ncol = 3, nrow = 6))
names(Result) = c("perc.opt", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}")

Result$perc.opt = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.MOTRL10), print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.MOTRL10.a), print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))

Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.MOTRL10.b), print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))

Result
```

Normalization: No
```{r}
N<-500 # sample size of training data
N2<-1000 # sample size of test data
iter <- 10 # replication
w0 = 0.3


perc.TRL11 = perc.TRL12 = perc.MOTRL10 = perc.MOTRL11 = perc.MOTRL12 = perc.MOTRL13 = rep(NA,iter)
EYs.TRL11.a = EYs.TRL12.a = EYs.TRL11.b = EYs.TRL12.b = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
  X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1)
  pi12 <- exp(0.5*x5 - 0.5*x1)
  
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
  
  g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  
  # 3 models #
  ################# Objective 1 ####################
  # simulate stage 1 optimal g11.opt for reward1
  Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
 
  ################# Objective 2 ####################
  # simulate stage 1 optimal g12.opt for reward2
  # Y12 <- 2.37 + x4 - 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 1.5*(exp((A1 == 1)) -1) + rnorm(N,0,1) # 0.3最好
  
  Y12 <- 1.37 + x4 - 2*x5 + 1*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 2*(exp((A1 == 1)) -1) + rnorm(N,0,1)
  
  Y11 = rescale(Y11, to = c(0,5))
  Y12 = rescale(Y12, to = c(0,5))

  ############ Multi-Objective weighted-sum reward at stage 1 ##########
  # stage 1 outcome
  Ys1 = cbind(Y11, Y12)
  
  ################ Grow the trees ######################
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(Y11, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(Y12, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  
  w11 = c(w0, 1-w0)
  MOTRL10 = MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(i+10000)
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
  
  #new:
  g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  
  ####### stage 1 prediction #######
  # predict selection %
  g1.TRL11 = TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test))
  g1.TRL12 = TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test))
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL13))

  
                                        # counterfactual mean outcome for TRL
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL11.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL11 - g1.opt.test)^2)) - 3*(g1.TRL11 == 1) + rnorm(N2,0,1))
  EYs.TRL12.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 3*(g1.TRL12 == 1) + rnorm(N2,0,1))
  
  ##### reward 2  # 2.5
  # EYs.TRL11.b[i] = mean(2.37 + x4.test - 2*x5.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
  #   1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,1))
  # EYs.TRL12.b[i] = mean(2.37 + x4.test - 2*x5.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
  #   1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,1))
  
  EYs.TRL11.b[i] = mean(1.37 + x4.test - 2*x5.test + 1*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 2*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,1))
  EYs.TRL12.b[i] = mean(1.37 + x4.test - 2*x5.test + 1*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 2*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,1))
  
   
                                             # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL10.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL10, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL11.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL11, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL12, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL13.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL13, g1.opt.test, x4.test, x1.test))


  ##### reward 2
  # EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b, c) 
  #   {2.37 + a - 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
  #   g1.MOTRL10, g1.opt.test, x4.test, x5.test, x6.test))
  # EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b, c) 
  #   {2.37 + a - 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)},   
  #   g1.MOTRL11, g1.opt.test, x4.test, x5.test, x6.test))
  # EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b, c) 
  #   {2.37 + a - 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
  #   g1.MOTRL12, g1.opt.test, x4.test, x5.test, x6.test))
  # EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b, c) 
  #   {2.37 + a - 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
  #   g1.MOTRL13, g1.opt.test, x4.test, x5.test, x6.test))
  
  EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b, c) 
    {1.37 + a - 2*b + mean(1*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(2*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL10, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b, c) 
    {1.37 + a - 2*b + mean(1*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(2*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)},   
    g1.MOTRL11, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b, c) 
    {1.37 + a - 2*b + mean(1*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(2*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL12, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b, c) 
    {1.37 + a - 2*b + mean(1*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) + mean(2*(exp((unlist(x) == 1)) -1)) + rnorm(N2,0,1)}, 
    g1.MOTRL13, g1.opt.test, x4.test, x5.test, x6.test))
  
  # EYs.TRL11.a = rescale(EYs.TRL11.a, to = c(0,5))
  # EYs.TRL11.b = rescale(EYs.TRL11.b, to = c(0,5))
  # 
  # EYs.TRL12.a = rescale(EYs.TRL12.a, to = c(0,5))
  # EYs.TRL12.b = rescale(EYs.TRL12.b, to = c(0,5))
  # 
  # EYs.MOTRL10.a = rescale(EYs.MOTRL10.a, to = c(0,5))
  # EYs.MOTRL11.a = rescale(EYs.MOTRL11.a, to = c(0,5))
  # EYs.MOTRL12.a = rescale(EYs.MOTRL12.a, to = c(0,5))
  # EYs.MOTRL13.a = rescale(EYs.MOTRL13.a, to = c(0,5))
  # 
  # EYs.MOTRL10.b = rescale(EYs.MOTRL10.b, to = c(0,5))
  # EYs.MOTRL11.b = rescale(EYs.MOTRL11.b, to = c(0,5))
  # EYs.MOTRL12.b = rescale(EYs.MOTRL12.b, to = c(0,5))
  # EYs.MOTRL13.b = rescale(EYs.MOTRL13.b, to = c(0,5))

}

Result = data.frame(matrix(NA, ncol = 3, nrow = 6))
names(Result) = c("perc.opt", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}")

Result$perc.opt = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.MOTRL10), print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.MOTRL10.a), print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))

Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.MOTRL10.b), print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))

Result
```

(iii) Small sample size (N = 500), large noice (~N(0,5))
(iv) Large sample size (N = 1000), large noice (~N(0,5))

```{r}
N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 10 # replication

perc.TRL11 = perc.TRL12 = perc.MOTRL10 = perc.MOTRL11 = perc.MOTRL12 = perc.MOTRL13 = rep(NA,iter)
EYs.TRL11.a = EYs.TRL12.a = EYs.TRL11.b = EYs.TRL12.b = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
  X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1)
  pi12 <- exp(0.5*x5 - 0.5*x1)
  
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
  
  g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  
  # 3 models #
  ################# Objective 1 ####################
  # simulate stage 1 optimal g11.opt for reward1
  Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
 
  ################# Objective 2 ####################
  # simulate stage 1 optimal g12.opt for reward2
  Y12 <- 2.37 + x4 + 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 1.5*(exp((A1 == 1)) -1) + rnorm(N,0,5)

  ############## Objective 3 : hidden ####################
  # Y13 <- ??? (A1 == 2)*(x6 == "Yes") 
  

  ############ Multi-Objective weighted-sum reward at stage 1 ##########
  # stage 1 outcome
  Ys1 = cbind(Y11, Y12)
  
  ################ Grow the trees ######################
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(Y11, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(Y12, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  w0 = 0.3
  w11 = c(w0, 1-w0)
  MOTRL10 = MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(i+10000)
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
  
  #new:
  g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  
  ####### stage 1 prediction #######
  # predict selection %
  g1.TRL11 = TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test))
  g1.TRL12 = TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test))
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL13))

  
                                        # counterfactual mean outcome for TRL
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL11.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL11 - g1.opt.test)^2)) - 3*(g1.TRL11 == 1) + rnorm(N2,0,1))
  EYs.TRL12.a[i] = mean(exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 3*(g1.TRL12 == 1) + rnorm(N2,0,1))
  
  ##### reward 2  # 2.5
  EYs.TRL11.b[i] = mean(2.36 - x4.test + 2*x5.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,5))
  EYs.TRL12.b[i] = mean(2.36 - x4.test + 2*x5.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,5))
  
   
                                             # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL10.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(x == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL10, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL11.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(x == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL11, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL12.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(x == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL12, g1.opt.test, x4.test, x1.test))
  EYs.MOTRL13.a[i] = mean(mapply(function(x, y, a, b) {exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(x == 1)) + rnorm(N2,0,1)},
                                 g1.MOTRL13, g1.opt.test, x4.test, x1.test))


  ##### reward 2
  EYs.MOTRL10.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 - a + 2*b + mean(2*(x == 0)*(2*(y == 0) - 1)) + mean(1.5*(x == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((x == 1)) -1)) + rnorm(N2,0,5)}, 
    g1.MOTRL10, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL11.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 - a + 2*b + mean(2*(x == 0)*(2*(y == 0) - 1)) + mean(1.5*(x == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((x == 1)) -1)) + rnorm(N2,0,5)},   
    g1.MOTRL11, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL12.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 - a + 2*b + mean(2*(x == 0)*(2*(y == 0) - 1)) + mean(1.5*(x == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((x == 1)) -1)) + rnorm(N2,0,5)}, 
    g1.MOTRL12, g1.opt.test, x4.test, x5.test, x6.test))
  EYs.MOTRL13.b[i] = mean(mapply(function(x, y, a, b, c) 
    {2.37 - a + 2*b + mean(2*(x == 0)*(2*(y == 0) - 1)) + mean(1.5*(x == 2)*(2*(y ==2) -1)) + mean(1.5*(exp((x == 1)) -1)) + rnorm(N2,0,5)}, 
    g1.MOTRL13, g1.opt.test, x4.test, x5.test, x6.test))
}

Result = data.frame(matrix(NA, ncol = 3, nrow = 6))
names(Result) = c("perc.opt", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}")

Result$perc.opt = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.MOTRL10), print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.MOTRL10.a), print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))

Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.MOTRL10.b), print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))

Result
```

getting opt and opt.sd
from Figure2Data100.csv -> 2D1stage.RData
```{r}
N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 100 # replication
weights2D1stages = read_csv("Figure2Data100.csv")
weights = weights2D1stages[,1]

Result = data.frame(matrix(NA, ncol = 2, nrow = 21))
names(Result) = c("perc.opt", "perc.opt.sd")

for (row in (1:nrow(weights2D1stages))) {
  perc.MOTRL12 = rep(NA,iter)
  
  for (i in 1:iter) {
    # Simulation begin
    set.seed(i+300)
    x1<-rnorm(N)              # each covariates follows N(0,1)
    x2<-rnorm(N)
    x3<-rnorm(N)
    x4<-rnorm(N)
    x5<-rnorm(N)
    x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
    X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates
  
    ############### stage 1 data simulation ##############
    # simulate A1, true stage 1 treatment with K1=3
    pi10 <- rep(1, N)
    pi11 <- exp(0.5*x4 + 0.5*x1)
    pi12 <- exp(0.5*x5 - 0.5*x1)
    
    # weights matrix
    matrix.pi1 <- cbind(pi10, pi11, pi12)
    A1 <- A.sim(matrix.pi1)
    class.A1 <- sort(unique(A1))
    # propensity stage 1
    pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
    
    g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
    
    # 3 models #
    ################# Objective 1 ####################
    # simulate stage 1 optimal g11.opt for reward1
    Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
   
    ################# Objective 2 ####################
    # simulate stage 1 optimal g12.opt for reward2
    Y12 <- 2.37 + x4 + 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 1.5*(exp((A1 == 1)) -1) + rnorm(N,0,1)
    
    # Y11 = rescale(Y11, to = c(0,5))
    # Y12 = rescale(Y12, to = c(0,5))
  
    ############ Multi-Objective weighted-sum reward at stage 1 ##########
    # stage 1 outcome
    Ys1 = cbind(Y11, Y12)

    w0 = as.numeric(weights[row,])
    w11 = c(w0,1-w0)
    MOTRL12 <- MO.tol.DTRtree(Ys1, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
    MOTRLtree12 = MOTRL12$tree
    
    
    ############################################
    # prediction using new data
    ############################################
    set.seed(i+10000)
    x1.test = rnorm(N2)
    x2.test = rnorm(N2)
    x3.test = rnorm(N2)
    x4.test = rnorm(N2)
    x5.test = rnorm(N2)
    x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
    X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
    
    #new:
    g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
    ####### stage 1 prediction #######
    # predict selection %
    g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  
    # percentage of true prediction
    perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
  }
  Result[row,] = c(mean(perc.MOTRL12), sd(perc.MOTRL12))
  weights2D1stages[row,2:3] = Result[row,]
}
  
save(weights2D1stages, file = "2D1stage.RData")
```

**Figure 2: For Scenario 1, n = 1000, e ~ N(0,1)**
***Figure 2.(a)***
The best are (0.45,0.55) and (0.5, 0.5)
```{r}
Figure2Data = read_csv("Figure2Data100.csv")
weight1 = Figure2Data$w0
weight2 = 1 - weight1
Obj1 <- data.frame(mean = Figure2Data$EY1, sd = Figure2Data$sd1)
Obj2 <- data.frame(mean = Figure2Data$EY2, sd = Figure2Data$sd2) 
error.crosses(Obj1, Obj2, sd=TRUE, 
              main = "Counterfactual mean outcomes for different weights",
              labels = mapply(function(x, y) {paste("(", x, ",", y, ")", sep = "")}, weight1, weight2),
              xlab = "E{Y_1*(gopt)}",
              ylab = "E{Y_2*(gopt)}", 
              xlim = c(-1, 5.3), ylim = c(2.5, 5.3), 
              arrow.len= 0.03,
              col.arrows="dark gray",
              pch=16,cex=0.9
              )
```
***Figure 2.(b) Bar plot for opt%***
```{r}
Figure2Data = read_csv("Figure2Data100.csv")

# (1) Line plot
ggplot(Figure2Data, aes(w0, opt)) +
  geom_line(aes(group = 1)) +
  geom_errorbar( aes(ymin = opt-opt.sd, ymax = opt+opt.sd), width = 0.02, colour = "gray") +
  geom_point(size = 2) + 
  theme_bw()+ theme(aspect.ratio=1/1.3) +
  labs(title="Percentage of optimization under different weights",
        x ="w1", y = "opt%") + theme(panel.grid.minor = element_blank())

ggplot(Figure2Data, aes(w0, opt)) +
  # geom_line(aes(group = 1)) +
  geom_errorbar( aes(ymin = opt-opt.sd, ymax = opt+opt.sd), width = 0.05, colour = "gray") +
  geom_point(size = 2) + 
  theme_bw()+ theme(aspect.ratio=1/1.3) +
  labs(title="Percentage of optimization under different weights",
        x ="w1", y = "opt%") + theme(panel.grid.minor = element_blank())

# (2) Bar plot
ggplot(Figure2Data, aes(w0, opt)) +
  geom_col(fill = "lightgray", color = "black",width = 0.035) +
  geom_errorbar(aes(ymin = opt - opt.sd, ymax = opt+opt.sd), width = 0.02) +
  theme_bw() + theme(aspect.ratio = 1/1.3) +
  labs(title="Percentage of optimization under different weights",
        x ="w1", y = "opt%") + theme(#panel.border = element_blank(),
                                     # panel.grid.major = element_blank()#,
                                     panel.grid.minor = element_blank()#,
                                     # axis.line = element_line(colour = "black")
                                     ) #+ 
  # scale_y_continuous(breaks= pretty_breaks()) +
  # scale_x_continuous(breaks= pretty_breaks())
```




## (B). Two stage, three treatments

**Final!**
```{r}
N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 100 # replication
w0 = 0.7
  
perc.TRL11 = perc.TRL12 = perc.TRL21 = perc.TRL22 = perc.TRLall1 = perc.TRLall2 =
  perc.MOTRL10 = perc.MOTRL20 = perc.MOTRLall0 =
  perc.MOTRL11 = perc.MOTRL21 = perc.MOTRLall1 =
  perc.MOTRL12 = perc.MOTRL22 = perc.MOTRLall2 =
  perc.MOTRL13 = perc.MOTRL23 = perc.MOTRLall3 = rep(NA,iter)


EYs.TRL11.a = EYs.TRL12.a = EYs.TRL11.b = EYs.TRL12.b = 
  EYs.MOTRL10.a = EYs.MOTRL10.b = 
  EYs.MOTRL11.a = EYs.MOTRL11.b = 
  EYs.MOTRL12.a = EYs.MOTRL12.b = 
  EYs.MOTRL13.a = EYs.MOTRL13.b = rep(NA,iter) 

EYs.TRL21.a = EYs.TRL22.a = EYs.TRL21.b = EYs.TRL22.b = 
  EYs.MOTRL20.a = EYs.MOTRL20.b = 
  EYs.MOTRL21.a = EYs.MOTRL21.b = 
  EYs.MOTRL22.a = EYs.MOTRL22.b = 
  EYs.MOTRL23.a = EYs.MOTRL23.b = rep(NA,iter) 

EYs.TRLall1.a = EYs.TRLall2.a = EYs.TRLall1.b = EYs.TRLall2.b = 
  EYs.MOTRLall0.a = EYs.MOTRLall0.b = 
  EYs.MOTRLall1.a = EYs.MOTRLall1.b = 
  EYs.MOTRLall2.a = EYs.MOTRLall2.b = 
  EYs.MOTRLall3.a = EYs.MOTRLall3.b = rep(NA,iter) # estimated mean counterfactual outcome

for (i in 1:iter) {
  # Simulation begin
  set.seed(i+300)
  x1<-rnorm(N)              # each covariates follows N(0,1)
  x2<-rnorm(N)
  x3<-rnorm(N)
  x4<-rnorm(N)
  x5<-rnorm(N)
  x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
  X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates

  ############### stage 1 data simulation ##############
  # simulate A1, true stage 1 treatment with K1=3
  pi10 <- rep(1, N)
  pi11 <- exp(0.5*x4 + 0.5*x1)
  pi12 <- exp(0.5*x5 - 0.5*x1)
  # weights matrix
  matrix.pi1 <- cbind(pi10, pi11, pi12)
  A1 <- A.sim(matrix.pi1)
  class.A1 <- sort(unique(A1))
  # propensity stage 1
  pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
  # True g1^opt
  g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
  ### Objective 1 
  Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
  ### Objective 2 
  Y12 <- 2.37 - x4 + 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 1.5*(exp((A1 == 1)) -1) + rnorm(N,0,1)
  Ys1 = cbind(Y11, Y12)
  
  
  ############### stage 2 data simulation ##############
  # A2, stage 2 treatment with K2 = 3
  pi20 <- rep(1,N)
  pi21 <- exp(0.2*Y12 - 0.5)
  pi22 <- exp(0.2*Y11 - 0.5*x2)
  matrix.pi2 <- cbind(pi20, pi21, pi22)
  A2 <- A.sim(matrix.pi2)
  class.A2 <- sort(unique(A2))
  # propensity stage 2
  pis2.hat<-M.propen(A2,cbind(Y11,Y12,x2))

  # True g2.opt
  # g2.opt <- (x3 > -1)*((Y11 > 0) + (Y12 > 2))
  # g2.opt <- (Y11 <= 2)*(Y12 > 3) + 2*(Y11 > 2)*(x3 > -1)
  
  g2.opt <- (x1 <= 0)*((x5 > -0.5) + (x5 > 1.5)) + 2*(x1 > 0)*(x3 > - 1)
  
  ### stage 2Objective 1 
  Y21 <- 1 + exp(1.5 + 0.2*x2 - abs(1.5*x3 + 2)*(A2 - g2.opt)^2) - 2*(A2 == 1) + rnorm(N,0,1)
  ### Objective 2 
  Y22 <- 1.7 + x2 + 2*(A2 == 0)*(2*(g2.opt == 0) - 1) + (A2 == 1)*(2*(g2.opt == 1) - 1) +
    3*(A2 == 2)*(2*(g2.opt == 2) - 1) + 1.2*(exp((A2 == 1)) -1) + rnorm(N,0,1)
  Ys2 = cbind(Y21, Y22)
  
  ############### stage 2 Estimation #####################
  #               Backward induction
  ########################################################
  
  # conditional mean model using linear regression 
  # REG21 <- TRL::Reg.mu(Y = Y21, As=cbind(A1,A2), H = cbind(X0,Y21))
  # mus21.reg<-REG21$mus.reg
  # REG22 <- TRL::Reg.mu(Y = Y22, As=cbind(A1,A2), H = cbind(X0,Y22))
  # mus22.reg<-REG22$mus.reg
  # DTRtree on reward1
  TRLtree21 <- TRL::DTRtree(Y21, A2, H=cbind(X0,as.factor(A1),Y11), pis.hat=pis2.hat, # mus.reg=mus21.reg,
                            lambda.pct=0.05, minsplit=max(0.05*N,20), depth = 3)
  # DTRtree on reward2
  TRLtree22 <- TRL::DTRtree(Y22, A2, H=cbind(X0,as.factor(A1),Y12), pis.hat=pis2.hat, # mus.reg=mus22.reg,
                            lambda.pct=0.1, minsplit=max(0.05*N,20),depth = 2)
  
  ############### stage 1 Estimation ################################
  # TRL
  # calculate pseudo outcome 1 (PO1)
  # expected optimal stage 2 outcome 
  E.Y21.tree <- rep(NA,N)
  # estimated optimal regime
  g2.TRLtree21 <- TRL::predict_DTR(TRLtree21,newdata=data.frame(X0,A1,Y11))
  ## use observed R2 + E(loss), modified Q learning as in Huang et al.2015
  # random forest for the estimated mean
  RF21 <- randomForest(Y21~., data = data.frame(A2,X0,A1,Y11))
  mus21.RF <- matrix(NA, N, length(class.A2))
  for(d in 1L:length(class.A2)) {
    mus21.RF[,d] <- predict(RF21,newdata=data.frame(A2=rep(class.A2[d],N),X0,A1,Y11))
  }
  for(m in 1:N){
    E.Y21.tree[m] <- RF21$predicted[m] + mus21.RF[m,g2.TRLtree21[m]+1] - mus21.RF[m,A2[m]+1]
  }
  # pseudo outcomes
  PO.TRLtree1 <- Y11 + E.Y21.tree
  
  # calculate pseudo outcome 2 (PO2)
  # expected optimal stage 2 outcome 
  E.Y22.tree <- rep(NA,N)
  # estimated optimal regime
  g2.TRLtree22 <- TRL::predict_DTR(TRLtree22,newdata=data.frame(X0,A1,Y12))
  ## use observed R2 + E(loss), modified Q learning as in Huang et al.2015
  # random forest for the estimated mean
  RF22 <- randomForest(Y22~., data = data.frame(A2,X0,A1,Y12))
  mus22.RF <- matrix(NA, N, length(class.A2))
  for(d in 1L:length(class.A2)) {
    mus22.RF[,d] <- predict(RF22,newdata=data.frame(A2=rep(class.A2[d],N),X0,A1,Y12))
  }
  for(m in 1:N){
    E.Y22.tree[m] <- RF22$predicted[m] + mus22.RF[m,g2.TRLtree22[m]+1] - mus22.RF[m,A2[m]+1]
  }
  # pseudo outcomes
  PO.TRLtree2 <- Y12 + E.Y22.tree
  
  # Stage 1
  # DTRtree on reward1
  TRLtree11 <- TRL::DTRtree(PO.TRLtree1, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  # DTRtree on reward2
  TRLtree12 <- TRL::DTRtree(PO.TRLtree2, A1, H=X0, pis.hat=pis1.hat, lambda.pct=0.05, minsplit=max(0.05*N,20))
  
  
  ############################## MOTRL #######################################
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,

  w21 = c(w0, 1-w0)
  MOTRL20 = MO.tol.DTRtree(Ys2, w = w21, A2, H=cbind(X0,A1,Ys1), delta = 0, pis.hat=pis1.hat, 
                           lambda.pct=0.02, minsplit=max(0.05*N,20),depth = 4)
  MOTRLtree20 = MOTRL20$tree
  MOTRL21 <- MO.tol.DTRtree(Ys2, w = w21, A2, H=cbind(X0,A1,Ys1), delta = 0.1, pis.hat=pis1.hat, 
                            lambda.pct=0.02, minsplit=max(0.05*N,20),depth = 4)
  MOTRLtree21 = MOTRL21$tree
  MOTRL22 <- MO.tol.DTRtree(Ys2, w = w21, A2, H=cbind(X0,A1,Ys1), delta = 0.3, pis.hat=pis1.hat, 
                            lambda.pct=0.02, minsplit=max(0.05*N,20),depth = 4)
  MOTRLtree22 = MOTRL22$tree
  MOTRL23 <- MO.tol.DTRtree(Ys2, w = w21, A2, H=cbind(X0,A1,Ys1), delta = 0.5, pis.hat=pis1.hat, 
                            lambda.pct=0.02, minsplit=max(0.05*N,20),depth = 4)
  MOTRLtree23 = MOTRL23$tree
  
  # calculate average pseudo outcome (avePO)
  g2.MOTRLtree20 = predict_tol.DTR(MOTRLtree20, newdata = cbind(X0,A1,Ys1))
  g2.MOTRLtree21 = predict_tol.DTR(MOTRLtree21, newdata = cbind(X0,A1,Ys1))
  g2.MOTRLtree22 = predict_tol.DTR(MOTRLtree22, newdata = cbind(X0,A1,Ys1))
  g2.MOTRLtree23 = predict_tol.DTR(MOTRLtree23, newdata = cbind(X0,A1,Ys1))
  # PO 
  # PO.MOTRL20 = Ys1 + Ys2 + MOTRL20$PO.loss
  # PO.MOTRL21 = Ys1 + Ys2 + MOTRL21$PO.loss
  # PO.MOTRL22 = Ys1 + Ys2 + MOTRL22$PO.loss
  # PO.MOTRL23 = Ys1 + Ys2 + MOTRL23$PO.loss
  
  PO.MOTRL20 = Ys1 + MOTRL20$PO.loss
  PO.MOTRL21 = Ys1 + MOTRL21$PO.loss
  PO.MOTRL22 = Ys1 + MOTRL22$PO.loss
  PO.MOTRL23 = Ys1 + MOTRL23$PO.loss
  
  # PO.MOTRL20 = PO.MOTRL21 = PO.MOTRL22 = PO.MOTRL23 = Ys1 
  
  # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
  w11 = w21
  MOTRL10 = MO.tol.DTRtree(PO.MOTRL20, w = w11, A1, H=X0, delta = 0, pis.hat=pis1.hat, 
                           lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree10 = MOTRL10$tree
  MOTRL11 <- MO.tol.DTRtree(PO.MOTRL21, w = w11, A1, H=X0, delta = 0.1,pis.hat=pis1.hat, 
                            lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree11 = MOTRL11$tree
  MOTRL12 <- MO.tol.DTRtree(PO.MOTRL22, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, 
                            lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree12 = MOTRL12$tree
  MOTRL13 <- MO.tol.DTRtree(PO.MOTRL23, w = w11, A1, H=X0, delta = 0.5,pis.hat=pis1.hat, 
                            lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
  MOTRLtree13 = MOTRL13$tree
  
  ############################################
  # prediction using new data
  ############################################
  set.seed(1000+i)
  
  x1.test = rnorm(N2)
  x2.test = rnorm(N2)
  x3.test = rnorm(N2)
  x4.test = rnorm(N2)
  x5.test = rnorm(N2)
  x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
  X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
  
  # true gopt at stage 1:
  g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
  
  # Y11.test <- exp(1.68 + 0.2*x4.test) + rnorm(N2,0,1)
  # Y12.test <- 2.37 - x4.test + 2*x5.test + rnorm(N2,0,1)
  
  ####### stage 1 prediction #######
  g1.TRL11 = TRL::predict_DTR(TRLtree11,newdata=data.frame(X0.test))
  g1.TRL12 = as.numeric(TRL::predict_DTR(TRLtree12,newdata=data.frame(X0.test)))
  g1.MOTRL10 = predict_tol.DTR(MOTRLtree10, newdata=data.frame(X0.test)) 
  g1.MOTRL11 = predict_tol.DTR(MOTRLtree11, newdata=data.frame(X0.test)) 
  g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  g1.MOTRL13 = predict_tol.DTR(MOTRLtree13, newdata=data.frame(X0.test)) 
  
  # good
  Y11.TRLtree1 = exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL11 - g1.opt.test)^2)) - 
    3*(g1.TRL11 == 1) + rnorm(N2,0,1)
  Y12.TRLtree1 = 2.37 - x4.test + 2*x5.test + 2*(g1.TRL11 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL11 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL11 == 1)) -1) + rnorm(N2,0,1)
  
  Y11.TRLtree2 = exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.TRL12 - g1.opt.test)^2)) - 
    3*(g1.TRL12 == 1) + rnorm(N2,0,1)
  # good
  Y12.TRLtree2 = 2.37 - x4.test + 2*x5.test + 2*(g1.TRL12 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.TRL12 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.TRL12 == 1)) -1) + rnorm(N2,0,1)
  
  
  EYs.TRL11.a[i] = mean(Y11.TRLtree1)
  EYs.TRL11.b[i] = mean(Y12.TRLtree1)
  EYs.TRL12.a[i] = mean(Y11.TRLtree2)
  EYs.TRL12.b[i] = mean(Y12.TRLtree2)
  
  
  # direct
  # Y11.MOTRLtree10 = exp(1.68 + 0.2*x4.test - abs(0.5*x1.test - 1)*((g1.MOTRL10 - g1.opt.test)^2)) - 3*(g1.MOTRL10 == 1) + rnorm(N2,0,1)
  # mapply
  Y11.MOTRLtree10 = c(mapply(function(x, y, a, b) {
    exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) 
    }, g1.MOTRL10, g1.opt.test, x4.test, x1.test)) + rnorm(N2,0,1)
  Y11.MOTRLtree11 = c(mapply(function(x, y, a, b) {
    exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) 
    }, g1.MOTRL11, g1.opt.test, x4.test, x1.test)) + rnorm(N2,0,1)
  Y11.MOTRLtree12 = c(mapply(function(x, y, a, b) {
    exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) 
    }, g1.MOTRL12, g1.opt.test, x4.test, x1.test)) + rnorm(N2,0,1)
  Y11.MOTRLtree13 = c(mapply(function(x, y, a, b) {
    exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) 
    }, g1.MOTRL13, g1.opt.test, x4.test, x1.test)) + rnorm(N2,0,1)
  
  # direct
  Y12.MOTRLtree10 = 2.37 - x4.test + 2*x5.test + 2*(g1.MOTRL10 == 0)*(2*(g1.opt.test == 0) - 1) +
    1.5*(g1.MOTRL10 == 2)*(2*(g1.opt.test ==2) - 1) + 1.5*(exp((g1.MOTRL10 == 1)) -1) + rnorm(N2,0,1)
  # mapply
  Y12.MOTRLtree11 = c(mapply(function(x, y, a, b) {
    2.37 - a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) +
      mean(1.5*(exp((unlist(x) == 1)) -1)) 
    }, g1.MOTRL11, g1.opt.test, x4.test, x5.test)) + rnorm(N2,0,1)
  Y12.MOTRLtree12 = c(mapply(function(x, y, a, b) {
    2.37 - a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) +
      mean(1.5*(exp((unlist(x) == 1)) -1)) 
    }, g1.MOTRL12, g1.opt.test, x4.test, x5.test)) + rnorm(N2,0,1)
  Y12.MOTRLtree13 = c(mapply(function(x, y, a, b) {
    2.37 - a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) +
      mean(1.5*(exp((unlist(x) == 1)) -1)) 
    }, g1.MOTRL13, g1.opt.test, x4.test, x5.test)) + rnorm(N2,0,1)
  
  EYs.MOTRL10.a[i] = mean(Y11.MOTRLtree10)
  EYs.MOTRL11.a[i] = mean(Y11.MOTRLtree11)
  EYs.MOTRL12.a[i] = mean(Y11.MOTRLtree12)
  EYs.MOTRL13.a[i] = mean(Y11.MOTRLtree13)
  
  EYs.MOTRL10.b[i] = mean(Y12.MOTRLtree10)
  EYs.MOTRL11.b[i] = mean(Y12.MOTRLtree11)
  EYs.MOTRL12.b[i] = mean(Y12.MOTRLtree12)
  EYs.MOTRL13.b[i] = mean(Y12.MOTRLtree13)

  
  # true gopt at stage 2:
  # g2.opt.test <- (x3.test > -1)*((as.numeric(Y11.MOTRLtree10) > 0) + (as.numeric(Y12.MOTRLtree10) > 2))
  # g2.opt.test <- (Y11.MOTRLtree10 <= 2)*(Y12.MOTRLtree10 > 3) + 2*(Y11.MOTRLtree10 > 2)*(x3.test > -1)
  # g2.opt.test <- (x1.test <= 0)*(x5.test > 0) + 2*(x1.test > 0)*(x3.test > - 1)
  # g2.opt.test <- (x1.test <= 0)*((x5.test > 0) + (x5.test > 1)) + 2*(x1.test > 0)*(x3.test > - 1) # 刚才的
  g2.opt.test <- (x1.test <= 0)*((x5.test > -0.5) + (x5.test > 1.5)) + 2*(x1.test > 0)*(x3.test > - 1)

  ####### stage 2 prediction #######
  g2.TRL21 <- TRL::predict_DTR(TRLtree21, newdata=data.frame(X0.test, A1 = g1.TRL11, Y11 = Y11.TRLtree1))
  g2.TRL22 <- TRL::predict_DTR(TRLtree22, newdata=data.frame(X0.test, A1 = g1.TRL12, Y12 = Y12.TRLtree2))
  
  # g2.MOTRL20 = predict_tol.DTR(MOTRLtree20, newdata=data.frame(X0.test, A1 = g1.MOTRL10, Ys1 = cbind(Y11.MOTRLtree10, Y12.MOTRLtree10))) 
  # g2.MOTRL21 = predict_tol.DTR(MOTRLtree21, newdata=data.frame(X0.test, A1 = g1.MOTRL11, Ys1 = cbind(Y11.MOTRLtree11, Y12.MOTRLtree11))) 
  # g2.MOTRL22 = predict_tol.DTR(MOTRLtree22, newdata=data.frame(X0.test, A1 = g1.MOTRL12, Ys1 = cbind(Y11.MOTRLtree12, Y12.MOTRLtree12))) 
  # g2.MOTRL23 = predict_tol.DTR(MOTRLtree23, newdata=data.frame(X0.test, A1 = g1.MOTRL13, Ys1 = cbind(Y11.MOTRLtree13, Y12.MOTRLtree13))) 
  
  g2.MOTRL20 = predict_tol.DTR(MOTRLtree20, newdata=data.frame(X0.test, A1 = g1.MOTRL10, cbind(Y11.MOTRLtree10, Y12.MOTRLtree10)))
  g2.MOTRL21 = predict_tol.DTR(MOTRLtree21, newdata=data.frame(X0.test, A1 = g1.MOTRL10, cbind(Y11.MOTRLtree11, Y12.MOTRLtree11))) 
  g2.MOTRL22 = predict_tol.DTR(MOTRLtree22, newdata=data.frame(X0.test, A1 = g1.MOTRL10, cbind(Y11.MOTRLtree12, Y12.MOTRLtree12))) 
  g2.MOTRL23 = predict_tol.DTR(MOTRLtree23, newdata=data.frame(X0.test, A1 = g1.MOTRL10, cbind(Y11.MOTRLtree13, Y12.MOTRLtree13))) 
  

  # percentage of true prediction
  # TRL
  perc.TRL11[i] = 100 * mean(g1.opt.test == g1.TRL11)
  perc.TRL12[i] = 100 * mean(g1.opt.test == g1.TRL12)
  perc.TRL21[i] = 100 * mean(g2.opt.test == g2.TRL21)
  perc.TRL22[i] = 100 * mean(g2.opt.test == g2.TRL22)
  perc.TRLall1[i] = 100 * mean(g1.opt.test == g1.TRL11 & g2.opt.test == g2.TRL21)
  perc.TRLall2[i] = 100 * mean(g1.opt.test == g1.TRL12 & g2.opt.test == g2.TRL22)
  # MOTRL
  perc.MOTRL10[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL10))
  perc.MOTRL11[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL11))
  perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
  perc.MOTRL13[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL13))
  
  perc.MOTRL20[i] = 100 * mean(mapply(function(x, y) x %in% y, g2.opt.test, g2.MOTRL20))
  perc.MOTRL21[i] = 100 * mean(mapply(function(x, y) x %in% y, g2.opt.test, g2.MOTRL21))
  perc.MOTRL22[i] = 100 * mean(mapply(function(x, y) x %in% y, g2.opt.test, g2.MOTRL22))
  perc.MOTRL23[i] = 100 * mean(mapply(function(x, y) x %in% y, g2.opt.test, g2.MOTRL23))
  
  perc.MOTRLall0[i] = 100 * mean(mapply(function(x, y, a, b) {(x %in% y) & (a %in% b)}, g1.opt.test, g1.MOTRL10, g2.opt.test, g2.MOTRL20))
  perc.MOTRLall1[i] = 100 * mean(mapply(function(x, y, a, b) {(x %in% y) & (a %in% b)}, g1.opt.test, g1.MOTRL11, g2.opt.test, g2.MOTRL21))
  perc.MOTRLall2[i] = 100 * mean(mapply(function(x, y, a, b) {(x %in% y) & (a %in% b)}, g1.opt.test, g1.MOTRL12, g2.opt.test, g2.MOTRL22))
  perc.MOTRLall3[i] = 100 * mean(mapply(function(x, y, a, b) {(x %in% y) & (a %in% b)}, g1.opt.test, g1.MOTRL13, g2.opt.test, g2.MOTRL23))
  
                                        # counterfactual mean outcome for TRL
  ##### reward 1
  # 对应Y11.1  # 4.48
  EYs.TRL21.a[i] = mean(1 + exp(1.5 + 0.2*x2.test - abs(1.5*x3.test + 2)*((g2.TRL21 - g2.opt.test)^2)) - 2*(g2.TRL21 == 1) + rnorm(N2,0,1)) 
  EYs.TRL22.a[i] = mean(1 + exp(1.5 + 0.2*x2.test - abs(1.5*x3.test + 2)*((g2.TRL22 - g2.opt.test)^2)) - 2*(g2.TRL22 == 1) + rnorm(N2,0,1)) 
  ##### reward 2  # 2.5
  EYs.TRL21.b[i] = mean(1.7 + x2.test + 2*(g2.TRL21 == 0)*(2*(g2.opt.test == 0) - 1) + (g2.TRL21 == 1)*(2*(g2.opt.test == 1) - 1) +
    3*(g2.TRL21 == 2)*(2*(g2.opt.test ==2) - 1) + 1.2*(exp((g2.TRL21 == 1)) -1) + rnorm(N2,0,1)) 
  
  EYs.TRL22.b[i] = mean(1.7 + x2.test + 2*(g2.TRL22 == 0)*(2*(g2.opt.test == 0) - 1) + (g2.TRL22 == 1)*(2*(g2.opt.test == 1) - 1) +
    3*(g2.TRL22 == 2)*(2*(g2.opt.test ==2) - 1) + 1.2*(exp((g2.TRL22 == 1)) -1) + rnorm(N2,0,1)) 

                                         # counterfactual mean outcome for MOTRL
  ##### reward 1
  EYs.MOTRL20.a[i] = mean(mapply(function(x, y, a, b) {1 + exp(1.5 + 0.2*a - abs(1.5*b + 2)*mean((unlist(x) - y)^2)) - 
      mean(2*(unlist(x) == 1)) + rnorm(N2,0,1)}, g2.MOTRL20, g2.opt.test, x2.test, x3.test)) 
  EYs.MOTRL21.a[i] = mean(mapply(function(x, y, a, b) {1 + exp(1.5 + 0.2*a - abs(1.5*b + 2)*mean((unlist(x) - y)^2)) - 
      mean(2*(unlist(x) == 1)) + rnorm(N2,0,1)}, g2.MOTRL21, g2.opt.test, x2.test, x3.test)) 
  EYs.MOTRL22.a[i] = mean(mapply(function(x, y, a, b) {1 + exp(1.5 + 0.2*a - abs(1.5*b + 2)*mean((unlist(x) - y)^2)) - 
      mean(2*(unlist(x) == 1)) + rnorm(N2,0,1)}, g2.MOTRL22, g2.opt.test, x2.test, x3.test)) 
  EYs.MOTRL23.a[i] = mean(mapply(function(x, y, a, b) {1 + exp(1.5 + 0.2*a - abs(1.5*b + 2)*mean((unlist(x) - y)^2)) - 
      mean(2*(unlist(x) == 1)) + rnorm(N2,0,1)}, g2.MOTRL23, g2.opt.test, x2.test, x3.test)) 
  
  ##### reward 2
  EYs.MOTRL20.b[i] = mean(mapply(function(x, y, z) 
    {1.7 + z + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean((unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(3*(unlist(x) == 2)*(2*(y == 2) - 1)) + mean(1.2*exp((unlist(x) == 1)) -1) + rnorm(N2,0,1)}, 
    g2.MOTRL20, g2.opt.test, x2.test)) 
  
  EYs.MOTRL21.b[i] = mean(mapply(function(x, y, z) 
    {1.7 + z + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean((unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(3*(unlist(x) == 2)*(2*(y == 2) - 1)) + mean(1.2*exp((unlist(x) == 1)) -1) + rnorm(N2,0,1)}, 
    g2.MOTRL21, g2.opt.test, x2.test)) 
  
  EYs.MOTRL22.b[i] = mean(mapply(function(x, y, z) 
    {1.7 + z + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean((unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(3*(unlist(x) == 2)*(2*(y == 2) - 1)) + mean(1.2*exp((unlist(x) == 1)) -1) + rnorm(N2,0,1)}, 
    g2.MOTRL22, g2.opt.test, x2.test))
  
  EYs.MOTRL23.b[i] = mean(mapply(function(x, y, z) 
    {1.7 + z + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean((unlist(x) == 1)*(2*(y ==1) -1)) + 
      mean(3*(unlist(x) == 2)*(2*(y == 2) - 1)) + mean(1.2*exp((unlist(x) == 1)) -1) + rnorm(N2,0,1)}, 
    g2.MOTRL23, g2.opt.test, x2.test)) 
  
  
  EYs.TRLall1.a[i] = EYs.TRL21.a[i] + mean(Y11.TRLtree1)
  EYs.TRLall2.a[i] = EYs.TRL22.a[i] + mean(Y11.TRLtree2)
  EYs.TRLall1.b[i] = EYs.TRL21.b[i] + mean(Y12.TRLtree1)
  EYs.TRLall2.b[i] = EYs.TRL22.b[i] + mean(Y12.TRLtree2)
  
  EYs.MOTRLall0.a[i] = EYs.MOTRL20.a[i] + + mean(Y11.MOTRLtree10)
  EYs.MOTRLall1.a[i] = EYs.MOTRL21.a[i] + + mean(Y11.MOTRLtree11)
  EYs.MOTRLall2.a[i] = EYs.MOTRL22.a[i] + + mean(Y11.MOTRLtree12)
  EYs.MOTRLall3.a[i] = EYs.MOTRL23.a[i] + + mean(Y11.MOTRLtree13)
  
  EYs.MOTRLall0.b[i] = EYs.MOTRL20.b[i] + mean(Y12.MOTRLtree10)
  EYs.MOTRLall1.b[i] = EYs.MOTRL21.b[i] + mean(Y12.MOTRLtree11)
  EYs.MOTRLall2.b[i] = EYs.MOTRL22.b[i] + mean(Y12.MOTRLtree12)
  EYs.MOTRLall3.b[i] = EYs.MOTRL23.b[i] + mean(Y12.MOTRLtree13)
}


Result = data.frame(matrix(NA, ncol = 9, nrow = 6))
names(Result) = c("perc.opt1", "mE{R11*(g_hat)}", "mE{R12*(g_hat)}",
                  "perc.opt2", "mE{R21*(g_hat)}", "mE{R22*(g_hat)}",
                  "perc.opt.all", "mE{Y1*(g_hat)}", "mE{Y2*(g_hat)}")

Result$perc.opt1 = c(print.summary(perc.TRL11), print.summary(perc.TRL12), print.summary(perc.MOTRL10), 
                     print.summary(perc.MOTRL11), print.summary(perc.MOTRL12), print.summary(perc.MOTRL13))
Result$perc.opt2 = c(print.summary(perc.TRL21), print.summary(perc.TRL22), print.summary(perc.MOTRL20), 
                     print.summary(perc.MOTRL21), print.summary(perc.MOTRL22), print.summary(perc.MOTRL23))
Result$perc.opt.all = c(print.summary(perc.TRLall1), print.summary(perc.TRLall2), print.summary(perc.MOTRLall0), 
                        print.summary(perc.MOTRLall1), print.summary(perc.MOTRLall2), print.summary(perc.MOTRLall3))

Result$`mE{R11*(g_hat)}` = c(print.summary(EYs.TRL11.a), print.summary(EYs.TRL12.a), print.summary(EYs.MOTRL10.a), 
                            print.summary(EYs.MOTRL11.a), print.summary(EYs.MOTRL12.a), print.summary(EYs.MOTRL13.a))
Result$`mE{R12*(g_hat)}` = c(print.summary(EYs.TRL11.b), print.summary(EYs.TRL12.b), print.summary(EYs.MOTRL10.b), 
                            print.summary(EYs.MOTRL11.b), print.summary(EYs.MOTRL12.b), print.summary(EYs.MOTRL13.b))


Result$`mE{R21*(g_hat)}` = c(print.summary(EYs.TRL21.a), print.summary(EYs.TRL22.a), print.summary(EYs.MOTRL20.a), 
                            print.summary(EYs.MOTRL21.a), print.summary(EYs.MOTRL22.a), print.summary(EYs.MOTRL23.a))
Result$`mE{R22*(g_hat)}` = c(print.summary(EYs.TRL21.b), print.summary(EYs.TRL22.b), print.summary(EYs.MOTRL20.b), 
                            print.summary(EYs.MOTRL21.b), print.summary(EYs.MOTRL22.b), print.summary(EYs.MOTRL23.b))

Result$`mE{Y1*(g_hat)}` = c(print.summary(EYs.TRLall1.a), print.summary(EYs.TRLall2.a), print.summary(EYs.MOTRLall0.a), 
                            print.summary(EYs.MOTRLall1.a), print.summary(EYs.MOTRLall2.a), print.summary(EYs.MOTRLall3.a))
Result$`mE{Y2*(g_hat)}` = c(print.summary(EYs.TRLall1.b), print.summary(EYs.TRLall2.b), print.summary(EYs.MOTRLall0.b), 
                            print.summary(EYs.MOTRLall1.b), print.summary(EYs.MOTRLall2.b), print.summary(EYs.MOTRLall3.b))

Result
```

from Figure3Data100.csv -> 2D2stage.RData
```{r}
weights2D2stages = read_csv("Figure3Data100.csv")
weights = weights2D2stages[,1]

N<-1000 # sample size of training data
N2<-1000 # sample size of test data
iter <- 100 # replication

Result = data.frame(matrix(NA, ncol = 18, nrow = nrow(weights2D2stages)))
names(Result) = c("perc.opt1", "perc.opt1.sd", "mE{R11*(g_hat)}", "R11.sd", "mE{R12*(g_hat)}", "R12.sd",
                  "perc.opt2", "perc.opt2.sd", "mE{R21*(g_hat)}", "R21.sd", "mE{R22*(g_hat)}", "R22.sd",
                  "perc.opt.all", "perc.opt.all.sd", "mE{Y1*(g_hat)}", "Y1.sd", "mE{Y2*(g_hat)}", "Y2.sd")

for (row in (1:nrow(weights2D2stages))) {
  perc.MOTRL12 = perc.MOTRL22 = perc.MOTRLall2 = rep(NA,iter)
  EYs.MOTRL12.a = EYs.MOTRL12.b = EYs.MOTRL22.a = EYs.MOTRL22.b = EYs.MOTRLall2.a = EYs.MOTRLall2.b = rep(NA,iter) 
  for (i in 1:iter) {
    # Simulation begin
    set.seed(i+300)
    x1<-rnorm(N)              # each covariates follows N(0,1)
    x2<-rnorm(N)
    x3<-rnorm(N)
    x4<-rnorm(N)
    x5<-rnorm(N)
    x6<-answer(N, x = c("No", "Yes"), name = "Smoke")
    X0<-cbind(x1,x2,x3,x4,x5,x6) # All of the covariates
  
    ############### stage 1 data simulation ##############
    # simulate A1, true stage 1 treatment with K1=3
    pi10 <- rep(1, N)
    pi11 <- exp(0.5*x4 + 0.5*x1)
    pi12 <- exp(0.5*x5 - 0.5*x1)
    # weights matrix
    matrix.pi1 <- cbind(pi10, pi11, pi12)
    A1 <- A.sim(matrix.pi1)
    class.A1 <- sort(unique(A1))
    # propensity stage 1
    pis1.hat <- M.propen(A1, cbind(x1,x4,x5))
    # True g1^opt
    g1.opt <- (x2 <= 0.5)*(x1 > 0.5) + 2*(x2 > 0.5)*(x1 > -1)
    ### Objective 1 
    Y11 <- exp(1.68 + 0.2*x4 - abs(0.5*x1 - 1)*((A1 - g1.opt)^2)) - 3*(A1 == 1) + rnorm(N,0,1)
    ### Objective 2 
    Y12 <- 2.37 - x4 + 2*x5 + 2*(A1 == 0)*(2*(g1.opt == 0) - 1) + 1.5*(A1 == 2)*(2*(g1.opt == 2) -1) + 
      1.5*(exp((A1 == 1)) -1) + rnorm(N,0,1)
    Ys1 = cbind(Y11, Y12)
    
    ############### stage 2 data simulation ##############
    # A2, stage 2 treatment with K2 = 3
    pi20 <- rep(1,N)
    pi21 <- exp(0.2*Y12 - 0.5)
    pi22 <- exp(0.2*Y11 - 0.5*x2)
    matrix.pi2 <- cbind(pi20, pi21, pi22)
    A2 <- A.sim(matrix.pi2)
    class.A2 <- sort(unique(A2))
    # propensity stage 2
    pis2.hat<-M.propen(A2,cbind(Y11,Y12,x2))
    g2.opt <- (x1 <= 0)*((x5 > -0.5) + (x5 > 1.5)) + 2*(x1 > 0)*(x3 > - 1)
    
    ### stage 2Objective 1 
    Y21 <- 1 + exp(1.5 + 0.2*x2 - abs(1.5*x3 + 2)*(A2 - g2.opt)^2) - 2*(A2 == 1) + rnorm(N,0,1)
    ### Objective 2 
    Y22 <- 1.7 + x2 + 2*(A2 == 0)*(2*(g2.opt == 0) - 1) + (A2 == 1)*(2*(g2.opt == 1) - 1) +
      3*(A2 == 2)*(2*(g2.opt == 2) - 1) + 1.2*(exp((A2 == 1)) -1) + rnorm(N,0,1)
    Ys2 = cbind(Y21, Y22)
    
    ############### stage 2 Estimation #####################
    #               Backward induction
    ########################################################
    w0 = as.numeric(weights[row,])
    w21 = c(w0,1-w0)
    MOTRL22 <- MO.tol.DTRtree(Ys2, w = w21, A2, H=cbind(X0,A1,Ys1), delta = 0.3, pis.hat=pis1.hat, 
                              lambda.pct=0.02, minsplit=max(0.05*N,20),depth = 4)
    MOTRLtree22 = MOTRL22$tree
    # calculate average pseudo outcome (avePO)
    g2.MOTRLtree22 = predict_tol.DTR(MOTRLtree22, newdata = cbind(X0,A1,Ys1))
    PO.MOTRL22 = Ys1 + MOTRL22$PO.loss
  
   
    # MODTRtree on overall reward (with tolerant rate 100%, 90%, 70%, 50%,
    w11 = w21
    MOTRL12 <- MO.tol.DTRtree(PO.MOTRL22, w = w11, A1, H=X0, delta = 0.3,pis.hat=pis1.hat, 
                              lambda.pct=0.05, minsplit=max(0.05*N,20),depth = 3)
    MOTRLtree12 = MOTRL12$tree
    
    
    ############################################
    # prediction using new data
    ############################################
    set.seed(1000+i)
    
    x1.test = rnorm(N2)
    x2.test = rnorm(N2)
    x3.test = rnorm(N2)
    x4.test = rnorm(N2)
    x5.test = rnorm(N2)
    x6.test = answer(N2, x = c("No", "Yes"), name = "Smoke")
    X0.test = cbind(x1.test,x2.test,x3.test,x4.test,x5.test,x6.test) # All of the covariates
    
    # true gopt at stage 1:
    g1.opt.test <- (x2.test <= 0.5)*(x1.test > 0.5) + 2*(x2.test > 0.5)*(x1.test > -1)
    
    # Y11.test <- exp(1.68 + 0.2*x4.test) + rnorm(N2,0,1)
    # Y12.test <- 2.37 - x4.test + 2*x5.test + rnorm(N2,0,1)
    
    ####### stage 1 prediction #######
    g1.MOTRL12 = predict_tol.DTR(MOTRLtree12, newdata=data.frame(X0.test)) 
  
    Y11.MOTRLtree12 = c(mapply(function(x, y, a, b) {
      exp(1.68 + 0.2*a - abs(0.5*b - 1)*mean((unlist(x) - y)^2)) - mean(3*(unlist(x) == 1)) 
      }, g1.MOTRL12, g1.opt.test, x4.test, x1.test)) + rnorm(N2,0,1)
    Y12.MOTRLtree12 = c(mapply(function(x, y, a, b) {
      2.37 - a + 2*b + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean(1.5*(unlist(x) == 2)*(2*(y ==2) -1)) +
        mean(1.5*(exp((unlist(x) == 1)) -1)) 
      }, g1.MOTRL12, g1.opt.test, x4.test, x5.test)) + rnorm(N2,0,1)
    
    EYs.MOTRL12.a[i] = mean(Y11.MOTRLtree12)
    EYs.MOTRL12.b[i] = mean(Y12.MOTRLtree12)
  
    # true gopt at stage 2:
    g2.opt.test <- (x1.test <= 0)*((x5.test > -0.5) + (x5.test > 1.5)) + 2*(x1.test > 0)*(x3.test > - 1)
    ####### stage 2 prediction #######
    g2.MOTRL22 = predict_tol.DTR(MOTRLtree22, newdata=data.frame(X0.test, A1 = g1.MOTRL10, cbind(Y11.MOTRLtree12, Y12.MOTRLtree12))) 
    
  
    # percentage of true prediction
    # MOTRL
    perc.MOTRL12[i] = 100 * mean(mapply(function(x, y) x %in% y, g1.opt.test, g1.MOTRL12))
    perc.MOTRL22[i] = 100 * mean(mapply(function(x, y) x %in% y, g2.opt.test, g2.MOTRL22))
    perc.MOTRLall2[i] = 100 * mean(mapply(function(x, y, a, b) {(x %in% y) & (a %in% b)}, g1.opt.test, g1.MOTRL12, g2.opt.test, g2.MOTRL22))
    
                                          # counterfactual mean outcome for TRL
    ##### reward 1
    EYs.MOTRL22.a[i] = mean(mapply(function(x, y, a, b) {1 + exp(1.5 + 0.2*a - abs(1.5*b + 2)*mean((unlist(x) - y)^2)) - 
        mean(2*(unlist(x) == 1)) + rnorm(N2,0,1)}, g2.MOTRL22, g2.opt.test, x2.test, x3.test)) 
    ##### reward 2
    EYs.MOTRL22.b[i] = mean(mapply(function(x, y, z) 
      {1.7 + z + mean(2*(unlist(x) == 0)*(2*(y == 0) - 1)) + mean((unlist(x) == 1)*(2*(y ==1) -1)) + 
        mean(3*(unlist(x) == 2)*(2*(y == 2) - 1)) + mean(1.2*exp((unlist(x) == 1)) -1) + rnorm(N2,0,1)}, 
      g2.MOTRL22, g2.opt.test, x2.test))
    
    
    EYs.MOTRLall2.a[i] = EYs.MOTRL22.a[i] + mean(Y11.MOTRLtree12)
    EYs.MOTRLall2.b[i] = EYs.MOTRL22.b[i] + mean(Y12.MOTRLtree12)
  }
  Result[row,] = c(mean(perc.MOTRL12), sd(perc.MOTRL12), 
                   mean(EYs.MOTRL12.a), sd(EYs.MOTRL12.a), 
                   mean(EYs.MOTRL12.b), sd(EYs.MOTRL12.b), 
                   
                   mean(perc.MOTRL22), sd(perc.MOTRL22), 
                   mean(EYs.MOTRL22.a), sd(EYs.MOTRL22.a), 
                   mean(EYs.MOTRL22.b), sd(EYs.MOTRL22.b),
                   
                   mean(perc.MOTRLall2), sd(perc.MOTRLall2), 
                   mean(EYs.MOTRLall2.a), sd(EYs.MOTRLall2.a), 
                   mean(EYs.MOTRLall2.b), sd(EYs.MOTRLall2.b))
  weights2D2stages[row,2:19] = Result[row,]
  
}


save(weights2D2stages, file = "2D2stages.RData")
```

## Figure 3: For Scenario 2, n = 1000, e ~ N(0,1)

```{r}
Figure3Data = weights2D2stages
weight1 = Figure3Data$w0
weight2 = 1 - weight1

# (a). stage 1
Obj11 <- data.frame(mean = Figure3Data$ER11, sd = Figure3Data$sdR11)
Obj12 <- data.frame(mean = Figure3Data$ER12, sd = Figure3Data$sdR12)
error.crosses(Obj11, Obj12, sd=TRUE,
              main = "Counterfactual mean rewards for different weights at stage 1",
              labels = mapply(function(x, y) {paste("(", x, ",", y, ")", sep = "")}, weight1, weight2),
              xlab = "E{R_11*(gopt)}",
              ylab = "E{R_12*(gopt)}",
              xlim = c(-0.6, 5.2), ylim = c(3.0, 5.1),
              arrow.len= 0.03,
              col.arrows="dark gray",
              pch=16,cex=0.9
              )  


# (b). stage 2
Obj21 <- data.frame(mean = Figure3Data$ER21, sd = Figure3Data$sdR21)
Obj22 <- data.frame(mean = Figure3Data$ER22, sd = Figure3Data$sdR22)
error.crosses(Obj21, Obj22, sd=TRUE, 
              main = "Counterfactual mean rewards for different weights at stage 2",
              labels = mapply(function(x, y) {paste("(", x, ",", y, ")", sep = "")}, weight1, weight2),
              xlab = "E{R_21*(gopt)}",
              ylab = "E{R_22*(gopt)}", 
              # xlim = c(3.6, 5), ylim = c(2.5, 5), 
              xlim = c(3.6, 5), ylim = c(2.5, 5),
              arrow.len= 0.03,
              col.arrows="dark gray",
              pch=16,cex=0.9
              ) 

# (c). overall
Obj1 <- data.frame(mean = Figure3Data$EY1, sd = Figure3Data$sdY1)
Obj2 <- data.frame(mean = Figure3Data$EY2, sd = Figure3Data$sdY2) 
error.crosses(Obj1, Obj2, sd=TRUE, 
              main = "Counterfactual mean outcomes for different weights",
              labels = mapply(function(x, y) {paste("(", x, ",", y, ")", sep = "")}, weight1, weight2),
              xlab = "E{Y_1*(gopt)}",
              ylab = "E{Y_2*(gopt)}", 
              xlim = c(3.0, 10.0), ylim = c(5.9, 10), 
              arrow.len= 0.03,
              col.arrows="dark gray",
              pch=16,cex=0.9
              ) 




# Obj1 <- data.frame(mean = Figure2Data$EY1, sd = Figure2Data$sd1)
# Obj2 <- data.frame(mean = Figure2Data$EY2, sd = Figure2Data$sd2)
# error.crosses(Obj1, Obj2, sd=TRUE,
#               main = "Counterfactual mean outcomes for different weights",
#               labels = mapply(function(x, y) {paste("(", x, ",", y, ")", sep = "")}, weight1, weight2),
#               xlab = "E{Y_1*(gopt)}",
#               ylab = "E{Y_2*(gopt)}",
#               xlim = c(-1, 5.3), ylim = c(2.5, 5.3),
#               arrow.len= 0.03,
#               col.arrows="dark gray",
#               pch=16,cex=0.9
#               )

```








